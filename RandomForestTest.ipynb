{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a11c743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "697459d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38788578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6782a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb1c1bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f49be3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.special import kl_div\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1f0aaf",
   "metadata": {},
   "source": [
    "In this notebook we will be testing code to construct the random forest using python libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f266802",
   "metadata": {},
   "source": [
    "The first step will be reading in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2aa90f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Motif_present = pd.read_csv('/home/robbenm/LuberLab/crispri/EcoBisTest/Motif_Present.tsv',sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eeebf62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_id</th>\n",
       "      <th>ec_id</th>\n",
       "      <th>ko_id</th>\n",
       "      <th>map_id</th>\n",
       "      <th>species</th>\n",
       "      <th>MEME-1</th>\n",
       "      <th>MEME-4</th>\n",
       "      <th>MEME-5</th>\n",
       "      <th>MEME-10</th>\n",
       "      <th>MEME-7</th>\n",
       "      <th>...</th>\n",
       "      <th>MEME-42</th>\n",
       "      <th>MEME-87</th>\n",
       "      <th>MEME-62</th>\n",
       "      <th>MEME-81</th>\n",
       "      <th>MEME-80</th>\n",
       "      <th>MEME-79</th>\n",
       "      <th>MEME-46</th>\n",
       "      <th>MEME-94</th>\n",
       "      <th>MEME-61</th>\n",
       "      <th>MEME-84</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bis:DXK01_000010</td>\n",
       "      <td>ec:5.4.99.2</td>\n",
       "      <td>ko:K01847</td>\n",
       "      <td>['path:map00280', 'path:map00630', 'path:map00...</td>\n",
       "      <td>bis</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bis:DXK01_000015</td>\n",
       "      <td>ec:5.4.99.2</td>\n",
       "      <td>ko:K01847</td>\n",
       "      <td>['path:map00280', 'path:map00630', 'path:map00...</td>\n",
       "      <td>bis</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bis:DXK01_000035</td>\n",
       "      <td>ec:6.1.1.6</td>\n",
       "      <td>ko:K04567</td>\n",
       "      <td>[path:map00970]</td>\n",
       "      <td>bis</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bis:DXK01_000040</td>\n",
       "      <td>ec:1.1.1.94</td>\n",
       "      <td>ko:K00057</td>\n",
       "      <td>['path:map00564', 'path:map01110']</td>\n",
       "      <td>bis</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bis:DXK01_000045</td>\n",
       "      <td>ec:5.3.1.9</td>\n",
       "      <td>ko:K01810</td>\n",
       "      <td>['path:map00010', 'path:map00030', 'path:map00...</td>\n",
       "      <td>bis</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gene_id        ec_id      ko_id  \\\n",
       "0  bis:DXK01_000010  ec:5.4.99.2  ko:K01847   \n",
       "1  bis:DXK01_000015  ec:5.4.99.2  ko:K01847   \n",
       "2  bis:DXK01_000035   ec:6.1.1.6  ko:K04567   \n",
       "3  bis:DXK01_000040  ec:1.1.1.94  ko:K00057   \n",
       "4  bis:DXK01_000045   ec:5.3.1.9  ko:K01810   \n",
       "\n",
       "                                              map_id species  MEME-1  MEME-4  \\\n",
       "0  ['path:map00280', 'path:map00630', 'path:map00...     bis   False    True   \n",
       "1  ['path:map00280', 'path:map00630', 'path:map00...     bis   False   False   \n",
       "2                                    [path:map00970]     bis   False   False   \n",
       "3                 ['path:map00564', 'path:map01110']     bis   False   False   \n",
       "4  ['path:map00010', 'path:map00030', 'path:map00...     bis   False   False   \n",
       "\n",
       "   MEME-5  MEME-10  MEME-7  ...  MEME-42  MEME-87  MEME-62  MEME-81  MEME-80  \\\n",
       "0   False    False   False  ...    False    False    False    False    False   \n",
       "1   False     True   False  ...    False    False    False    False    False   \n",
       "2   False    False   False  ...    False    False    False    False     True   \n",
       "3   False    False   False  ...    False    False    False    False    False   \n",
       "4   False     True   False  ...    False    False    False    False    False   \n",
       "\n",
       "   MEME-79  MEME-46  MEME-94  MEME-61  MEME-84  \n",
       "0    False    False    False    False    False  \n",
       "1    False    False    False    False    False  \n",
       "2    False    False    False    False    False  \n",
       "3    False    False    False    False    False  \n",
       "4    False    False    False    False    False  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Motif_present.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9437dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Motif_number = pd.read_csv('/home/robbenm/LuberLab/crispri/EcoBisTest/Motif_Number.tsv',sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3667a932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_id</th>\n",
       "      <th>ec_id</th>\n",
       "      <th>ko_id</th>\n",
       "      <th>map_id</th>\n",
       "      <th>species</th>\n",
       "      <th>MEME-1</th>\n",
       "      <th>MEME-4</th>\n",
       "      <th>MEME-5</th>\n",
       "      <th>MEME-10</th>\n",
       "      <th>MEME-7</th>\n",
       "      <th>...</th>\n",
       "      <th>MEME-42</th>\n",
       "      <th>MEME-87</th>\n",
       "      <th>MEME-62</th>\n",
       "      <th>MEME-81</th>\n",
       "      <th>MEME-80</th>\n",
       "      <th>MEME-79</th>\n",
       "      <th>MEME-46</th>\n",
       "      <th>MEME-94</th>\n",
       "      <th>MEME-61</th>\n",
       "      <th>MEME-84</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eco:b3994</td>\n",
       "      <td>ec:4.1.99.17</td>\n",
       "      <td>ko:K03147</td>\n",
       "      <td>['path:map00730', 'path:map01100']</td>\n",
       "      <td>eco</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bis:DXK01_001240</td>\n",
       "      <td>ec:4.1.99.17</td>\n",
       "      <td>ko:K03147</td>\n",
       "      <td>['path:map00730', 'path:map01100']</td>\n",
       "      <td>bis</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bis:DXK01_013610</td>\n",
       "      <td>ec:1.7.99.1</td>\n",
       "      <td>ko:K05601</td>\n",
       "      <td>['path:map00910', 'path:map01100']</td>\n",
       "      <td>bis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bis:DXK01_015090</td>\n",
       "      <td>ec:5.4.2.11</td>\n",
       "      <td>ko:K01834</td>\n",
       "      <td>['path:map00010', 'path:map00260', 'path:map00...</td>\n",
       "      <td>bis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eco:b0755</td>\n",
       "      <td>ec:5.4.2.11</td>\n",
       "      <td>ko:K01834</td>\n",
       "      <td>['path:map00010', 'path:map00260', 'path:map00...</td>\n",
       "      <td>eco</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gene_id         ec_id      ko_id  \\\n",
       "0         eco:b3994  ec:4.1.99.17  ko:K03147   \n",
       "1  bis:DXK01_001240  ec:4.1.99.17  ko:K03147   \n",
       "2  bis:DXK01_013610   ec:1.7.99.1  ko:K05601   \n",
       "3  bis:DXK01_015090   ec:5.4.2.11  ko:K01834   \n",
       "4         eco:b0755   ec:5.4.2.11  ko:K01834   \n",
       "\n",
       "                                              map_id species  MEME-1  MEME-4  \\\n",
       "0                 ['path:map00730', 'path:map01100']     eco       1       1   \n",
       "1                 ['path:map00730', 'path:map01100']     bis       1       1   \n",
       "2                 ['path:map00910', 'path:map01100']     bis       0       1   \n",
       "3  ['path:map00010', 'path:map00260', 'path:map00...     bis       0       0   \n",
       "4  ['path:map00010', 'path:map00260', 'path:map00...     eco       0       0   \n",
       "\n",
       "   MEME-5  MEME-10  MEME-7  ...  MEME-42  MEME-87  MEME-62  MEME-81  MEME-80  \\\n",
       "0       0        1       0  ...        0        0        0        0        0   \n",
       "1       0        1       0  ...        0        0        0        0        0   \n",
       "2       1        0       0  ...        0        0        0        0        0   \n",
       "3       0        0       1  ...        0        0        0        0        0   \n",
       "4       0        0       1  ...        0        0        0        0        0   \n",
       "\n",
       "   MEME-79  MEME-46  MEME-94  MEME-61  MEME-84  \n",
       "0        0        0        0        0        0  \n",
       "1        0        0        0        0        0  \n",
       "2        0        1        0        0        0  \n",
       "3        0        0        0        0        0  \n",
       "4        0        0        0        0        0  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Motif_number.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e86e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nodes_present = pd.read_csv('/home/robbenm/LuberLab/crispri/EcoBisTest/Nodes_Present.tsv',sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be1bcbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_id</th>\n",
       "      <th>ec_id</th>\n",
       "      <th>ko_id</th>\n",
       "      <th>map_id</th>\n",
       "      <th>species</th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>n3</th>\n",
       "      <th>n4</th>\n",
       "      <th>n5</th>\n",
       "      <th>...</th>\n",
       "      <th>n3151</th>\n",
       "      <th>n3152</th>\n",
       "      <th>n3153</th>\n",
       "      <th>n3154</th>\n",
       "      <th>n3155</th>\n",
       "      <th>n3156</th>\n",
       "      <th>n3157</th>\n",
       "      <th>n3158</th>\n",
       "      <th>n3159</th>\n",
       "      <th>n3160</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bis:DXK01_000010</td>\n",
       "      <td>ec:5.4.99.2</td>\n",
       "      <td>ko:K01847</td>\n",
       "      <td>['path:map00280', 'path:map00630', 'path:map00...</td>\n",
       "      <td>bis</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bis:DXK01_000015</td>\n",
       "      <td>ec:5.4.99.2</td>\n",
       "      <td>ko:K01847</td>\n",
       "      <td>['path:map00280', 'path:map00630', 'path:map00...</td>\n",
       "      <td>bis</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bis:DXK01_000035</td>\n",
       "      <td>ec:6.1.1.6</td>\n",
       "      <td>ko:K04567</td>\n",
       "      <td>[path:map00970]</td>\n",
       "      <td>bis</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bis:DXK01_000040</td>\n",
       "      <td>ec:1.1.1.94</td>\n",
       "      <td>ko:K00057</td>\n",
       "      <td>['path:map00564', 'path:map01110']</td>\n",
       "      <td>bis</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bis:DXK01_000045</td>\n",
       "      <td>ec:5.3.1.9</td>\n",
       "      <td>ko:K01810</td>\n",
       "      <td>['path:map00010', 'path:map00030', 'path:map00...</td>\n",
       "      <td>bis</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3165 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gene_id        ec_id      ko_id  \\\n",
       "0  bis:DXK01_000010  ec:5.4.99.2  ko:K01847   \n",
       "1  bis:DXK01_000015  ec:5.4.99.2  ko:K01847   \n",
       "2  bis:DXK01_000035   ec:6.1.1.6  ko:K04567   \n",
       "3  bis:DXK01_000040  ec:1.1.1.94  ko:K00057   \n",
       "4  bis:DXK01_000045   ec:5.3.1.9  ko:K01810   \n",
       "\n",
       "                                              map_id species     n1     n2  \\\n",
       "0  ['path:map00280', 'path:map00630', 'path:map00...     bis   True  False   \n",
       "1  ['path:map00280', 'path:map00630', 'path:map00...     bis  False   True   \n",
       "2                                    [path:map00970]     bis  False  False   \n",
       "3                 ['path:map00564', 'path:map01110']     bis  False  False   \n",
       "4  ['path:map00010', 'path:map00030', 'path:map00...     bis  False  False   \n",
       "\n",
       "      n3     n4     n5  ...  n3151  n3152  n3153  n3154  n3155  n3156  n3157  \\\n",
       "0  False  False  False  ...  False  False  False  False  False  False  False   \n",
       "1  False  False  False  ...  False  False  False  False  False  False  False   \n",
       "2   True  False  False  ...  False  False  False  False  False  False  False   \n",
       "3  False   True  False  ...  False  False  False  False  False  False  False   \n",
       "4  False  False   True  ...  False  False  False  False  False  False  False   \n",
       "\n",
       "   n3158  n3159  n3160  \n",
       "0  False  False  False  \n",
       "1  False  False  False  \n",
       "2  False  False  False  \n",
       "3  False  False  False  \n",
       "4  False  False  False  \n",
       "\n",
       "[5 rows x 3165 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nodes_present.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2653f281",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nodes_number = pd.read_csv('/home/robbenm/LuberLab/crispri/EcoBisTest/Nodes_Number.tsv',sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a45acdfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_id</th>\n",
       "      <th>ec_id</th>\n",
       "      <th>ko_id</th>\n",
       "      <th>map_id</th>\n",
       "      <th>species</th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>n3</th>\n",
       "      <th>n4</th>\n",
       "      <th>n5</th>\n",
       "      <th>...</th>\n",
       "      <th>n3151</th>\n",
       "      <th>n3152</th>\n",
       "      <th>n3153</th>\n",
       "      <th>n3154</th>\n",
       "      <th>n3155</th>\n",
       "      <th>n3156</th>\n",
       "      <th>n3157</th>\n",
       "      <th>n3158</th>\n",
       "      <th>n3159</th>\n",
       "      <th>n3160</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bis:DXK01_000010</td>\n",
       "      <td>ec:5.4.99.2</td>\n",
       "      <td>ko:K01847</td>\n",
       "      <td>['path:map00280', 'path:map00630', 'path:map00...</td>\n",
       "      <td>bis</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bis:DXK01_000015</td>\n",
       "      <td>ec:5.4.99.2</td>\n",
       "      <td>ko:K01847</td>\n",
       "      <td>['path:map00280', 'path:map00630', 'path:map00...</td>\n",
       "      <td>bis</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bis:DXK01_000035</td>\n",
       "      <td>ec:6.1.1.6</td>\n",
       "      <td>ko:K04567</td>\n",
       "      <td>[path:map00970]</td>\n",
       "      <td>bis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bis:DXK01_000040</td>\n",
       "      <td>ec:1.1.1.94</td>\n",
       "      <td>ko:K00057</td>\n",
       "      <td>['path:map00564', 'path:map01110']</td>\n",
       "      <td>bis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bis:DXK01_000045</td>\n",
       "      <td>ec:5.3.1.9</td>\n",
       "      <td>ko:K01810</td>\n",
       "      <td>['path:map00010', 'path:map00030', 'path:map00...</td>\n",
       "      <td>bis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3165 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gene_id        ec_id      ko_id  \\\n",
       "0  bis:DXK01_000010  ec:5.4.99.2  ko:K01847   \n",
       "1  bis:DXK01_000015  ec:5.4.99.2  ko:K01847   \n",
       "2  bis:DXK01_000035   ec:6.1.1.6  ko:K04567   \n",
       "3  bis:DXK01_000040  ec:1.1.1.94  ko:K00057   \n",
       "4  bis:DXK01_000045   ec:5.3.1.9  ko:K01810   \n",
       "\n",
       "                                              map_id species  n1  n2  n3  n4  \\\n",
       "0  ['path:map00280', 'path:map00630', 'path:map00...     bis   6   0   0   0   \n",
       "1  ['path:map00280', 'path:map00630', 'path:map00...     bis   0   5   0   0   \n",
       "2                                    [path:map00970]     bis   0   0  13   0   \n",
       "3                 ['path:map00564', 'path:map01110']     bis   0   0   0   5   \n",
       "4  ['path:map00010', 'path:map00030', 'path:map00...     bis   0   0   0   0   \n",
       "\n",
       "   n5  ...  n3151  n3152  n3153  n3154  n3155  n3156  n3157  n3158  n3159  \\\n",
       "0   0  ...      0      0      0      0      0      0      0      0      0   \n",
       "1   0  ...      0      0      0      0      0      0      0      0      0   \n",
       "2   0  ...      0      0      0      0      0      0      0      0      0   \n",
       "3   0  ...      0      0      0      0      0      0      0      0      0   \n",
       "4   4  ...      0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "   n3160  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 3165 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nodes_number.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a51e7c",
   "metadata": {},
   "source": [
    "As we can see we have 4 different training data sets that either show gene information of Motifs or Nodes and the Presence or Number of motifs in each. We will be testing these four data sets against eachother to reveal which is the most accurate predictor of Pathway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9198bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_id</th>\n",
       "      <th>ec_id</th>\n",
       "      <th>ko_id</th>\n",
       "      <th>map_id</th>\n",
       "      <th>species</th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>n3</th>\n",
       "      <th>n4</th>\n",
       "      <th>n5</th>\n",
       "      <th>...</th>\n",
       "      <th>n3151</th>\n",
       "      <th>n3152</th>\n",
       "      <th>n3153</th>\n",
       "      <th>n3154</th>\n",
       "      <th>n3155</th>\n",
       "      <th>n3156</th>\n",
       "      <th>n3157</th>\n",
       "      <th>n3158</th>\n",
       "      <th>n3159</th>\n",
       "      <th>n3160</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bis:DXK01_000010</td>\n",
       "      <td>ec:5.4.99.2</td>\n",
       "      <td>ko:K01847</td>\n",
       "      <td>['path:map00280', 'path:map00630', 'path:map00...</td>\n",
       "      <td>bis</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bis:DXK01_000015</td>\n",
       "      <td>ec:5.4.99.2</td>\n",
       "      <td>ko:K01847</td>\n",
       "      <td>['path:map00280', 'path:map00630', 'path:map00...</td>\n",
       "      <td>bis</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bis:DXK01_000035</td>\n",
       "      <td>ec:6.1.1.6</td>\n",
       "      <td>ko:K04567</td>\n",
       "      <td>[path:map00970]</td>\n",
       "      <td>bis</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bis:DXK01_000040</td>\n",
       "      <td>ec:1.1.1.94</td>\n",
       "      <td>ko:K00057</td>\n",
       "      <td>['path:map00564', 'path:map01110']</td>\n",
       "      <td>bis</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bis:DXK01_000045</td>\n",
       "      <td>ec:5.3.1.9</td>\n",
       "      <td>ko:K01810</td>\n",
       "      <td>['path:map00010', 'path:map00030', 'path:map00...</td>\n",
       "      <td>bis</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3165 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gene_id        ec_id      ko_id  \\\n",
       "0  bis:DXK01_000010  ec:5.4.99.2  ko:K01847   \n",
       "1  bis:DXK01_000015  ec:5.4.99.2  ko:K01847   \n",
       "2  bis:DXK01_000035   ec:6.1.1.6  ko:K04567   \n",
       "3  bis:DXK01_000040  ec:1.1.1.94  ko:K00057   \n",
       "4  bis:DXK01_000045   ec:5.3.1.9  ko:K01810   \n",
       "\n",
       "                                              map_id species     n1     n2  \\\n",
       "0  ['path:map00280', 'path:map00630', 'path:map00...     bis   True  False   \n",
       "1  ['path:map00280', 'path:map00630', 'path:map00...     bis  False   True   \n",
       "2                                    [path:map00970]     bis  False  False   \n",
       "3                 ['path:map00564', 'path:map01110']     bis  False  False   \n",
       "4  ['path:map00010', 'path:map00030', 'path:map00...     bis  False  False   \n",
       "\n",
       "      n3     n4     n5  ...  n3151  n3152  n3153  n3154  n3155  n3156  n3157  \\\n",
       "0  False  False  False  ...  False  False  False  False  False  False  False   \n",
       "1  False  False  False  ...  False  False  False  False  False  False  False   \n",
       "2   True  False  False  ...  False  False  False  False  False  False  False   \n",
       "3  False   True  False  ...  False  False  False  False  False  False  False   \n",
       "4  False  False   True  ...  False  False  False  False  False  False  False   \n",
       "\n",
       "   n3158  n3159  n3160  \n",
       "0  False  False  False  \n",
       "1  False  False  False  \n",
       "2  False  False  False  \n",
       "3  False  False  False  \n",
       "4  False  False  False  \n",
       "\n",
       "[5 rows x 3165 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tree_present = pd.read_csv('/home/robbenm/LuberLab/crispri/EcoBisTest/Tree_Present.tsv',sep=\"\\t\")\n",
    "Tree_present.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1a60975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data \n",
      "Motif Present: (1523, 102) \n",
      "Motif Numbers: (1523, 102) \n",
      "Nodes Present: (1581, 3165) \n",
      "Nodes Numbers: (1581, 3165)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of the data \\nMotif Present:',Motif_present.shape,'\\nMotif Numbers:',Motif_number.shape,'\\nNodes Present:',Nodes_present.shape,'\\nNodes Numbers:', Nodes_number.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be30521",
   "metadata": {},
   "source": [
    "It seems that there are about 50 missing genes in Motif dataset, this is likely because no motifs could be identified in those genes but they still pathway annotations. Regardless we will keep that data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "302268b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>n3</th>\n",
       "      <th>n4</th>\n",
       "      <th>n5</th>\n",
       "      <th>n6</th>\n",
       "      <th>n7</th>\n",
       "      <th>n8</th>\n",
       "      <th>n9</th>\n",
       "      <th>n10</th>\n",
       "      <th>...</th>\n",
       "      <th>n3151</th>\n",
       "      <th>n3152</th>\n",
       "      <th>n3153</th>\n",
       "      <th>n3154</th>\n",
       "      <th>n3155</th>\n",
       "      <th>n3156</th>\n",
       "      <th>n3157</th>\n",
       "      <th>n3158</th>\n",
       "      <th>n3159</th>\n",
       "      <th>n3160</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1581.000000</td>\n",
       "      <td>1581.000000</td>\n",
       "      <td>1581.000000</td>\n",
       "      <td>1581.000000</td>\n",
       "      <td>1581.000000</td>\n",
       "      <td>1581.000000</td>\n",
       "      <td>1581.000000</td>\n",
       "      <td>1581.000000</td>\n",
       "      <td>1581.000000</td>\n",
       "      <td>1581.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1581.00000</td>\n",
       "      <td>1581.000000</td>\n",
       "      <td>1581.000000</td>\n",
       "      <td>1581.000000</td>\n",
       "      <td>1581.000000</td>\n",
       "      <td>1581.000000</td>\n",
       "      <td>1581.000000</td>\n",
       "      <td>1581.000000</td>\n",
       "      <td>1581.000000</td>\n",
       "      <td>1581.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.003163</td>\n",
       "      <td>0.008223</td>\n",
       "      <td>0.003163</td>\n",
       "      <td>0.002530</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.001898</td>\n",
       "      <td>0.005060</td>\n",
       "      <td>0.003163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00506</td>\n",
       "      <td>0.006325</td>\n",
       "      <td>0.006325</td>\n",
       "      <td>0.002530</td>\n",
       "      <td>0.006325</td>\n",
       "      <td>0.001265</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.001265</td>\n",
       "      <td>0.006325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.150899</td>\n",
       "      <td>0.125749</td>\n",
       "      <td>0.326947</td>\n",
       "      <td>0.125749</td>\n",
       "      <td>0.100599</td>\n",
       "      <td>0.025150</td>\n",
       "      <td>0.075449</td>\n",
       "      <td>0.201198</td>\n",
       "      <td>0.125749</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.17782</td>\n",
       "      <td>0.177779</td>\n",
       "      <td>0.181305</td>\n",
       "      <td>0.079515</td>\n",
       "      <td>0.181305</td>\n",
       "      <td>0.035556</td>\n",
       "      <td>0.128224</td>\n",
       "      <td>0.106668</td>\n",
       "      <td>0.050300</td>\n",
       "      <td>0.177779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 3160 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                n1           n2           n3           n4           n5  \\\n",
       "count  1581.000000  1581.000000  1581.000000  1581.000000  1581.000000   \n",
       "mean      0.003795     0.003163     0.008223     0.003163     0.002530   \n",
       "std       0.150899     0.125749     0.326947     0.125749     0.100599   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       6.000000     5.000000    13.000000     5.000000     4.000000   \n",
       "\n",
       "                n6           n7           n8           n9     n10  ...  \\\n",
       "count  1581.000000  1581.000000  1581.000000  1581.000000  1581.0  ...   \n",
       "mean      0.000633     0.001898     0.005060     0.003163     0.0  ...   \n",
       "std       0.025150     0.075449     0.201198     0.125749     0.0  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.0  ...   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.0  ...   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.0  ...   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.0  ...   \n",
       "max       1.000000     3.000000     8.000000     5.000000     0.0  ...   \n",
       "\n",
       "            n3151        n3152        n3153        n3154        n3155  \\\n",
       "count  1581.00000  1581.000000  1581.000000  1581.000000  1581.000000   \n",
       "mean      0.00506     0.006325     0.006325     0.002530     0.006325   \n",
       "std       0.17782     0.177779     0.181305     0.079515     0.181305   \n",
       "min       0.00000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.00000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.00000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.00000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       7.00000     5.000000     6.000000     3.000000     6.000000   \n",
       "\n",
       "             n3156        n3157        n3158        n3159        n3160  \n",
       "count  1581.000000  1581.000000  1581.000000  1581.000000  1581.000000  \n",
       "mean      0.001265     0.003795     0.003795     0.001265     0.006325  \n",
       "std       0.035556     0.128224     0.106668     0.050300     0.177779  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "max       1.000000     5.000000     3.000000     2.000000     5.000000  \n",
       "\n",
       "[8 rows x 3160 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nodes_number.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62f9fe68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gene_id    object\n",
       "ec_id      object\n",
       "ko_id      object\n",
       "map_id     object\n",
       "species    object\n",
       "            ...  \n",
       "MEME-79      bool\n",
       "MEME-46      bool\n",
       "MEME-94      bool\n",
       "MEME-61      bool\n",
       "MEME-84      bool\n",
       "Length: 102, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Motif_present.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bca37e7",
   "metadata": {},
   "source": [
    "Looks like there is not too much going on with the nodes, they are pretty random, no patterns, and the present dfs are bools so no need to one hot encode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c35692",
   "metadata": {},
   "source": [
    "Now let us do several things, first convert each of the training sets into numpy arrays to work with the thing. Then we want to separate the features and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f5ff6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1523"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Motif_present_targetEC = np.array(Motif_present['ec_id']) #Lets start with predicting the EC and then we can try mapID later\n",
    "Motif_number_targetEC = np.array(Motif_number['ec_id'])\n",
    "Nodes_present_targetEC = np.array(Nodes_present['ec_id'])\n",
    "Nodes_number_targetEC = np.array(Nodes_number['ec_id'])\n",
    "Tree_present_targetEC = np.array(Tree_present['ec_id'])\n",
    "Motif_number_targetEC.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c967ef0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1523"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Motif_present_targetMap = np.array(Motif_present['map_id'])\n",
    "Motif_number_targetMap = np.array(Motif_number['map_id'])\n",
    "Nodes_present_targetMap = np.array(Nodes_present['map_id'])\n",
    "Nodes_number_targetMap = np.array(Nodes_number['map_id'])\n",
    "Tree_present_targetMap = np.array(Tree_present['map_id'])\n",
    "Motif_present_targetMap.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec261325",
   "metadata": {},
   "outputs": [],
   "source": [
    "colDrop = ['gene_id','ec_id','ko_id','map_id','species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b078964",
   "metadata": {},
   "outputs": [],
   "source": [
    "Motif_present_features = Motif_present.drop(colDrop,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "558e64d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MEME-1</th>\n",
       "      <th>MEME-4</th>\n",
       "      <th>MEME-5</th>\n",
       "      <th>MEME-10</th>\n",
       "      <th>MEME-7</th>\n",
       "      <th>MEME-11</th>\n",
       "      <th>MEME-24</th>\n",
       "      <th>MEME-6</th>\n",
       "      <th>MEME-17</th>\n",
       "      <th>MEME-13</th>\n",
       "      <th>...</th>\n",
       "      <th>MEME-42</th>\n",
       "      <th>MEME-87</th>\n",
       "      <th>MEME-62</th>\n",
       "      <th>MEME-81</th>\n",
       "      <th>MEME-80</th>\n",
       "      <th>MEME-79</th>\n",
       "      <th>MEME-46</th>\n",
       "      <th>MEME-94</th>\n",
       "      <th>MEME-61</th>\n",
       "      <th>MEME-84</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MEME-1  MEME-4  MEME-5  MEME-10  MEME-7  MEME-11  MEME-24  MEME-6  MEME-17  \\\n",
       "0   False    True   False    False   False    False    False    True    False   \n",
       "1   False   False   False     True   False    False    False   False    False   \n",
       "2   False   False   False    False   False    False     True   False    False   \n",
       "3   False   False   False    False   False    False     True   False    False   \n",
       "4   False   False   False     True   False    False    False   False    False   \n",
       "\n",
       "   MEME-13  ...  MEME-42  MEME-87  MEME-62  MEME-81  MEME-80  MEME-79  \\\n",
       "0    False  ...    False    False    False    False    False    False   \n",
       "1    False  ...    False    False    False    False    False    False   \n",
       "2    False  ...    False    False    False    False     True    False   \n",
       "3    False  ...    False    False    False    False    False    False   \n",
       "4    False  ...    False    False    False    False    False    False   \n",
       "\n",
       "   MEME-46  MEME-94  MEME-61  MEME-84  \n",
       "0    False    False    False    False  \n",
       "1    False    False    False    False  \n",
       "2    False    False    False    False  \n",
       "3    False    False    False    False  \n",
       "4    False    False    False    False  \n",
       "\n",
       "[5 rows x 97 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Motif_present_features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "329e42b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Motif_number_features = Motif_number.drop(colDrop,axis = 1)\n",
    "Nodes_present_features = Nodes_present.drop(colDrop,axis = 1)\n",
    "Nodes_number_features = Nodes_number.drop(colDrop,axis = 1)\n",
    "Tree_present_features = Tree_present.drop(colDrop,axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b15f00",
   "metadata": {},
   "source": [
    "Lets also save the names of the features of each, why not (present and number feature names should be the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17005ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Motif_present_features_names = list(Motif_present_features.columns)\n",
    "Nodes_present_features_names = list(Nodes_present_features.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d52217",
   "metadata": {},
   "source": [
    "And now lastly we will convert the features into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe825013",
   "metadata": {},
   "outputs": [],
   "source": [
    "Motif_present_features_array = np.array(Motif_present_features)\n",
    "Motif_number_features_array = np.array(Motif_number_features)\n",
    "Nodes_present_features_array = np.array(Nodes_present_features)\n",
    "Nodes_number_features_array = np.array(Nodes_number_features)\n",
    "Tree_present_features_array = np.array(Tree_present_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bfb2e5",
   "metadata": {},
   "source": [
    "    Now we can start coding the random forest algorithm. Since we are just doing this to get a base on the random forest design, we will only use one dataframe 'Nodes_present' to code it and then we will do all with the cross validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1edb044",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(Motif_number_features_array,Motif_number_targetEC,test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8829d463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (1142, 97)\n",
      "Training Labels Shape: (1142,)\n",
      "Testing Features Shape: (381, 97)\n",
      "Testing Labels Shape: (381,)\n"
     ]
    }
   ],
   "source": [
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e48640",
   "metadata": {},
   "source": [
    "The tutorial mentions a baseline but we are predicting non-numeric so loss and accuracy is going to be the most important for us. Now we can import the regressor model and fit to our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9706f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf = RandomForestRegressor(n_estimators = 1000, random_state = 42) #think I need the classifier instead\n",
    "rf = RandomForestClassifier(n_estimators = 1000, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a22744",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(train_features,train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e1403d",
   "metadata": {},
   "source": [
    "It looks like we need to have removed the gene id column, which makes sense because otherwise it would think that gene ID is a predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60e8e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rf.predict(test_features)\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4972e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#caclulate the absolute errors \n",
    "# errors = abs(predictions - test_labels)\n",
    "errors = 0\n",
    "for x in range(0,len(predictions)):\n",
    "    if predictions[x] == test_labels[x]:\n",
    "        errors = errors + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401faff0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('accuracy was ',(errors/len(test_labels))*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a575647",
   "metadata": {},
   "source": [
    "Obviously that is very low but we aren't using any changing of parameters or anything and we are only using one model. \n",
    "For fun though, let's visualize one of the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0774d9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree = rf.estimators_[5]\n",
    "# export_graphviz(tree, out_file='/home/robbenm/LuberLab/crispri/EcoBisTest/testtree.dot',feature_names=Motif_present_features_names,rounded = True, precision = 1)\n",
    "# (graph,) = pydot.graph_from_dot_file('/home/robbenm/LuberLab/crispri/EcoBisTest/testtree.dot')\n",
    "# graph.write_png('/home/robbenm/LuberLab/crispri/EcoBisTest/testtree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e2acf1",
   "metadata": {},
   "source": [
    "nvm, the plotting does not work on here but I think I get it, let's move on and predict feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c783f75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importances = list(rf.feature_importances_)\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(Motif_present_features_names, importances)]\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b2229f",
   "metadata": {},
   "source": [
    "Good, at the very least no motif is more important than another in determining pathway"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ac35ab",
   "metadata": {},
   "source": [
    "Let's move on to the hyperparameter tuning with crossfold cv for this Eco-Bis set. Essentially the goal behind this is to have the algorithm scan through the different parameters while using different training/test sets to show us what parameters and training sets will provide the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61b632d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First make a new Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a147948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 42,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b33c2e",
   "metadata": {},
   "source": [
    "These are all of the parameters used by the random classifier to classify. We want to come up with a bunch of variables for each and put them in a grid so that the cv can iterate through them. The parameters we will use are:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9836295",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators =  [100, 500, 1000, 2500, 5000, 7500, 10000] # num of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9588198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = ['auto','sqrt'] # Number of features considered at each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "533c68f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = [int(x) for x in np.linspace(10,200,num = 10)] #max levels in each tree\n",
    "max_depth.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0064674",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_split = [2,5,10] # Min num samples to split a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b076f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_leaf = [1, 2, 4] # Min num samples required at each leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6e9cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee231e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 200, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [100, 500, 1000, 2500, 5000, 7500, 10000]}\n"
     ]
    }
   ],
   "source": [
    "random_grid = {'n_estimators': n_estimators,\n",
    "              'max_features': max_features,\n",
    "              'max_depth': max_depth,\n",
    "              'min_samples_split': min_samples_split,\n",
    "              'min_samples_leaf': min_samples_leaf,\n",
    "              'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169d0cd8",
   "metadata": {},
   "source": [
    "We will first do a random search that will randomly apply settings, because it is too computationally costly to do up to 4,320 runs of the model fitting, we would rather narrow it down first. we will continue to use just the Model_present at first but will need to create a function to easily plug in all of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "edafa950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "#Perform random search using 5 fold cv, accross 100 different settings using all available cores\n",
    "Motif_present_rf = RandomizedSearchCV(estimator=rf, \n",
    "                               param_distributions=random_grid,\n",
    "                               n_iter=100, \n",
    "                               cv = 5, \n",
    "                               verbose =2, \n",
    "                               random_state=42,\n",
    "                               n_jobs = 32)\n",
    "# Motif_present_rf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c685a6ea",
   "metadata": {},
   "source": [
    "It appears that when performing this multicored, each core can use up to 30 GB of memory, meaning for 16 threads you need at least 500 GB active. At 500 folds with 16 GB it took 30 min so doubling it would be adviseable for the 12+ runs I will have to do and larger data set. \n",
    "Any parallel error is due to lack of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d56a1052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motif_present_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ca099579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = Motif_present_rf.best_estimator_.predict(test_features)\n",
    "# len(predictions)\n",
    "# errors = 0\n",
    "# for x in range(0,len(predictions)):\n",
    "#     if predictions[x] == test_labels[x]:\n",
    "#         errors = errors + 1\n",
    "        \n",
    "# print('accuracy was ',(errors/len(test_labels))*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19f9555",
   "metadata": {},
   "source": [
    "Oof, we only increased accuracy by 1%, yikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b0d93b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we create a function that we can just input our table and it will completely make the model and test it.\n",
    "#we will not use the randomCV since right now we are just comparing but we will take the best options from\n",
    "#the previous randomCV and apply them to this\n",
    "def make_model(features,labels,ncores = 1):\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(features,\n",
    "                                                                                labels,\n",
    "                                                                                test_size = 0.25, \n",
    "                                                                                random_state = 42)\n",
    "    #Make the random forest\n",
    "    rf = RandomForestClassifier(random_state = 42,\n",
    "                                n_estimators = 5000,\n",
    "                                min_samples_split = 2,\n",
    "                                min_samples_leaf = 1,\n",
    "                                max_features = 'sqrt',\n",
    "                                max_depth = 10,\n",
    "                                bootstrap = True,\n",
    "                               oob_score = True)\n",
    "#     cv_score = cross_val_score(rf, features, labels,cv = 5,n_jobs = 16)\n",
    "    rf.fit(train_features,train_labels)\n",
    "    \n",
    "    \n",
    "    print('Training score: ', rf.score(train_features, train_labels))\n",
    "    print('OOB score: ', rf.oob_score_)\n",
    "    print('Test score: ', rf.score(test_features, test_labels))\n",
    "    \n",
    "    predictions = rf.predict(test_features)\n",
    "    y_pred = rf.predict_proba(test_features)\n",
    "    lloss = log_loss(test_labels,y_pred,labels=rf.classes_)\n",
    "    conf_mat = confusion_matrix(test_labels, predictions)\n",
    "    FP = conf_mat.sum(axis=0) - np.diag(conf_mat)  \n",
    "    FN = conf_mat.sum(axis=1) - np.diag(conf_mat)\n",
    "    TP = np.diag(conf_mat)\n",
    "    TN = conf_mat.sum() - (FP + FN + TP)\n",
    "\n",
    "#     # Sensitivity, hit rate, recall, or true positive rate\n",
    "#     TPR = TP/(TP+FN)\n",
    "#     # Specificity or true negative rate\n",
    "#     TNR = TN/(TN+FP) \n",
    "#     # Precision or positive predictive value\n",
    "#     PPV = TP/(TP+FP)\n",
    "#     # Negative predictive value\n",
    "#     NPV = TN/(TN+FN)\n",
    "#     # Fall out or false positive rate\n",
    "#     FPR = FP/(FP+TN)\n",
    "#     # False negative rate\n",
    "#     FNR = FN/(TP+FN)\n",
    "#     # False discovery rate\n",
    "#     FDR = FP/(TP+FP)\n",
    "\n",
    "#     # Overall accuracy\n",
    "#     ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "    \n",
    "    print('Average True Pos:',TP.mean())\n",
    "    print('Average True Neg:',TN.mean())\n",
    "    print('Average False Pos:',FP.mean())\n",
    "    print('Average False Neg:',FN.mean())\n",
    "#     print('True Positive Rate (recall):',TPR.mean())\n",
    "#     print('True Negative Rate:',TNR.mean())\n",
    "#     print('Precision:', PPV.mean())\n",
    "#     print('False Positive Rate:',FPR.mean())\n",
    "#     print('(meaningless) average computed accuracy:', ACC.mean())\n",
    "    \n",
    "    print('Log Loss: ',lloss)\n",
    "#     print('Cross validation score: ',cv_score.mean())\n",
    "    return(rf)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a50295",
   "metadata": {},
   "source": [
    "Now we can use the function to run through all 4 of our training data sets with 1) predicting EC and 2) predicting pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "125122e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  0.7898423817863398\n",
      "OOB score:  0.0446584938704028\n",
      "Test score:  0.06824146981627296\n",
      "Average True Pos: 0.07027027027027027\n",
      "Average True Neg: 379.0108108108108\n",
      "Average False Pos: 0.9594594594594594\n",
      "Average False Neg: 0.9594594594594594\n",
      "Log Loss:  3.865290615619425\n"
     ]
    }
   ],
   "source": [
    "Motif_present_rf = make_model(Motif_present_features_array,Motif_present_targetEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e3e536b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  0.7924693520140105\n",
      "OOB score:  0.05691768826619965\n",
      "Test score:  0.047244094488188976\n",
      "Average True Pos: 0.048\n",
      "Average True Neg: 379.016\n",
      "Average False Pos: 0.968\n",
      "Average False Neg: 0.968\n",
      "Log Loss:  3.984779055145796\n"
     ]
    }
   ],
   "source": [
    "Motif_number_rf = make_model(Motif_number_features_array,Motif_number_targetEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0720ad04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  0.47848101265822784\n",
      "OOB score:  0.1189873417721519\n",
      "Test score:  0.13131313131313133\n",
      "Average True Pos: 0.1452513966480447\n",
      "Average True Neg: 393.9329608938547\n",
      "Average False Pos: 0.9608938547486033\n",
      "Average False Neg: 0.9608938547486033\n",
      "Log Loss:  3.517048541623167\n"
     ]
    }
   ],
   "source": [
    "Nodes_present_rf = make_model(Nodes_present_features_array,Nodes_present_targetEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "203b78b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  0.41434599156118146\n",
      "OOB score:  0.08945147679324894\n",
      "Test score:  0.12121212121212122\n",
      "Average True Pos: 0.13793103448275862\n",
      "Average True Neg: 393.86206896551727\n",
      "Average False Pos: 1.0\n",
      "Average False Neg: 1.0\n",
      "Log Loss:  3.679297746178915\n"
     ]
    }
   ],
   "source": [
    "Nodes_number_rf = make_model(Nodes_number_features_array,Nodes_number_targetEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9bfb22cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree_present_rf = make_model(Tree_present_features_array,Tree_present_targetEC) #same results as nodes present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c2e087cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  0.7539404553415061\n",
      "OOB score:  0.05516637478108581\n",
      "Test score:  0.07349081364829396\n",
      "Average True Pos: 0.14432989690721648\n",
      "Average True Neg: 377.21649484536084\n",
      "Average False Pos: 1.8195876288659794\n",
      "Average False Neg: 1.8195876288659794\n",
      "Log Loss:  4.643495694732342\n"
     ]
    }
   ],
   "source": [
    "Motif_present_rfmap = make_model(Motif_present_features_array,Motif_present_targetMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "53490abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  0.7732049036777583\n",
      "OOB score:  0.053415061295971976\n",
      "Test score:  0.07349081364829396\n",
      "Average True Pos: 0.14893617021276595\n",
      "Average True Neg: 377.09574468085106\n",
      "Average False Pos: 1.877659574468085\n",
      "Average False Neg: 1.877659574468085\n",
      "Log Loss:  4.777378760201549\n"
     ]
    }
   ],
   "source": [
    "Motif_number_rfmap = make_model(Motif_number_features_array,Motif_number_targetMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e7159c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  0.4582278481012658\n",
      "OOB score:  0.1308016877637131\n",
      "Test score:  0.13383838383838384\n",
      "Average True Pos: 0.2535885167464115\n",
      "Average True Neg: 392.46411483253587\n",
      "Average False Pos: 1.6411483253588517\n",
      "Average False Neg: 1.6411483253588517\n",
      "Log Loss:  4.332446404368004\n"
     ]
    }
   ],
   "source": [
    "Nodes_present_rfmap = make_model(Nodes_present_features_array,Nodes_present_targetMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e3217b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  0.3848101265822785\n",
      "OOB score:  0.09367088607594937\n",
      "Test score:  0.11616161616161616\n",
      "Average True Pos: 0.2169811320754717\n",
      "Average True Neg: 392.4811320754717\n",
      "Average False Pos: 1.650943396226415\n",
      "Average False Neg: 1.650943396226415\n",
      "Log Loss:  4.427627830776111\n"
     ]
    }
   ],
   "source": [
    "Nodes_number_rfmap = make_model(Nodes_number_features_array,Nodes_number_targetMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5b5dfe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree_present_rfmap = make_model(Tree_present_features_array,Tree_present_targetMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef2a763",
   "metadata": {},
   "source": [
    "As it appears, predicting map IDs with nodes resulted in higher accuracy but that is when multiple map ID's are represented together, so if it doesn't predict all map IDs it will result in a loss of accuracy even if it gets one or two. Let's recreate the function above to work with separating and comparing individual map IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "886c0d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'path:map00280'\", \" 'path:map00630'\", \" 'path:map00640'\", \" 'path:map00720'\", \" 'path:map01100'\", \" 'path:map01120'\"]\n"
     ]
    }
   ],
   "source": [
    "#test code for splitting\n",
    "map1 = Motif_present['map_id'][1]#['path:map00280', 'path:map00630']\n",
    "map2 = [\"'path:map00280'\", \" 'path:map00630'\", \" 'path:map00640'\",]\n",
    "map1 = map1.replace('[','').replace(']','').split(',')\n",
    "print(map1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8f575ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_count(features,labels,ncores = 1):\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(features,\n",
    "                                                                                labels,\n",
    "                                                                                test_size = 0.25, \n",
    "                                                                                random_state = 42)\n",
    "    \n",
    "    #Make the random forest\n",
    "    rf = RandomForestClassifier(random_state = 42,\n",
    "                                n_estimators = 5000,\n",
    "                                min_samples_split = 2,\n",
    "                                min_samples_leaf = 1,\n",
    "                                max_features = 'sqrt',\n",
    "                                max_depth = 10,\n",
    "                                bootstrap = True)\n",
    "    rf.fit(train_features,train_labels)\n",
    "    predictions = rf.predict(test_features)\n",
    "    errors = 0\n",
    "    length = 0\n",
    "    less = 0\n",
    "    equal = 0\n",
    "    more = 0\n",
    "    for x in range(0,len(predictions)):\n",
    "        map1 = predictions[x]#['path:map00280', 'path:map00630']\n",
    "        map2 = test_labels[x]\n",
    "        map1 = map1.replace('[','').replace(']','').split(',')\n",
    "        map2 = map2.replace('[','').replace(']','').split(',')\n",
    "        total = len(set(map1).intersection(map2))\n",
    "        errors = errors + total\n",
    "        length = length + len(map2)\n",
    "        if (total < len(map2)): less = less + 1\n",
    "        if (total == len(map2)): equal = equal + 1\n",
    "        if (total < len(map1)): more = more + 1\n",
    "    print('accuracy was ',errors/length*100,'%')\n",
    "    print('in ', less/len(test_labels)*100,'% of genes, model did not predict all pathways')\n",
    "    print('in ', equal/len(test_labels)*100,'% of genes, model predicted all pathways')\n",
    "    print('in ', more/len(test_labels)*100,'% of genes, model predicted pathways not found in gene')\n",
    "    return(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5acd4c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy was  31.026438569206842 %\n",
      "in  92.6509186351706 % of genes, model did not predict all pathways\n",
      "in  7.349081364829396 % of genes, model predicted all pathways\n",
      "in  91.86351706036746 % of genes, model predicted pathways not found in gene\n"
     ]
    }
   ],
   "source": [
    "Motif_present_rfmapcount = make_model_count(Motif_present_features_array,Motif_present_targetMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1073ac9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy was  31.642512077294686 %\n",
      "in  92.6509186351706 % of genes, model did not predict all pathways\n",
      "in  7.349081364829396 % of genes, model predicted all pathways\n",
      "in  91.86351706036746 % of genes, model predicted pathways not found in gene\n"
     ]
    }
   ],
   "source": [
    "Motif_number_rfmapcount = make_model_count(Motif_number_features_array,Motif_number_targetMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "afbd1477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy was  18.62673484295106 %\n",
      "in  86.61616161616162 % of genes, model did not predict all pathways\n",
      "in  13.383838383838384 % of genes, model predicted all pathways\n",
      "in  86.61616161616162 % of genes, model predicted pathways not found in gene\n"
     ]
    }
   ],
   "source": [
    "Nodes_present_rfmapcount = make_model_count(Nodes_present_features_array,Nodes_present_targetMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1277df36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy was  15.997078159240322 %\n",
      "in  88.38383838383838 % of genes, model did not predict all pathways\n",
      "in  11.616161616161616 % of genes, model predicted all pathways\n",
      "in  88.38383838383838 % of genes, model predicted pathways not found in gene\n"
     ]
    }
   ],
   "source": [
    "Nodes_number_rfmapcount = make_model_count(Nodes_number_features_array,Nodes_number_targetMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5f32c340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Motif_Name</th>\n",
       "      <th>Motif_number_rf_EC</th>\n",
       "      <th>Motif_number_rf_Map</th>\n",
       "      <th>Motif_number_rf_Mapcount</th>\n",
       "      <th>Motif_present_rf_EC</th>\n",
       "      <th>Motif_present_rf_Map</th>\n",
       "      <th>Motif_present_rf_Mapcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MEME-1</td>\n",
       "      <td>0.010605</td>\n",
       "      <td>0.010807</td>\n",
       "      <td>0.010807</td>\n",
       "      <td>0.010569</td>\n",
       "      <td>0.011073</td>\n",
       "      <td>0.011073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MEME-4</td>\n",
       "      <td>0.008627</td>\n",
       "      <td>0.008157</td>\n",
       "      <td>0.008157</td>\n",
       "      <td>0.009029</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.009014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEME-5</td>\n",
       "      <td>0.009501</td>\n",
       "      <td>0.008852</td>\n",
       "      <td>0.008852</td>\n",
       "      <td>0.008883</td>\n",
       "      <td>0.008716</td>\n",
       "      <td>0.008716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MEME-10</td>\n",
       "      <td>0.009413</td>\n",
       "      <td>0.009890</td>\n",
       "      <td>0.009890</td>\n",
       "      <td>0.011041</td>\n",
       "      <td>0.010522</td>\n",
       "      <td>0.010522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MEME-7</td>\n",
       "      <td>0.009238</td>\n",
       "      <td>0.008730</td>\n",
       "      <td>0.008730</td>\n",
       "      <td>0.008753</td>\n",
       "      <td>0.008887</td>\n",
       "      <td>0.008887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Motif_Name  Motif_number_rf_EC  Motif_number_rf_Map  \\\n",
       "0     MEME-1            0.010605             0.010807   \n",
       "1     MEME-4            0.008627             0.008157   \n",
       "2     MEME-5            0.009501             0.008852   \n",
       "3    MEME-10            0.009413             0.009890   \n",
       "4     MEME-7            0.009238             0.008730   \n",
       "\n",
       "   Motif_number_rf_Mapcount  Motif_present_rf_EC  Motif_present_rf_Map  \\\n",
       "0                  0.010807             0.010569              0.011073   \n",
       "1                  0.008157             0.009029              0.009014   \n",
       "2                  0.008852             0.008883              0.008716   \n",
       "3                  0.009890             0.011041              0.010522   \n",
       "4                  0.008730             0.008753              0.008887   \n",
       "\n",
       "   Motif_present_rf_Mapcount  \n",
       "0                   0.011073  \n",
       "1                   0.009014  \n",
       "2                   0.008716  \n",
       "3                   0.010522  \n",
       "4                   0.008887  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Importance_data_Motifs = {'Motif_Name': Motif_present_features_names,\n",
    "                         'Motif_number_rf_EC': Motif_number_rf.feature_importances_,\n",
    "                         'Motif_number_rf_Map': Motif_number_rfmap.feature_importances_,\n",
    "                         'Motif_number_rf_Mapcount': Motif_number_rfmapcount.feature_importances_,\n",
    "                         'Motif_present_rf_EC': Motif_present_rf.feature_importances_,\n",
    "                         'Motif_present_rf_Map': Motif_present_rfmap.feature_importances_,\n",
    "                         'Motif_present_rf_Mapcount': Motif_present_rfmapcount.feature_importances_}\n",
    "Importance_data_Motifs_df = pd.DataFrame(Importance_data_Motifs)\n",
    "Importance_data_Motifs_df.to_csv('/home/robbenm/LuberLab/crispri/EcoBisTest/Motif_importance.csv')\n",
    "Importance_data_Motifs_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0c6d9c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nodes_Name</th>\n",
       "      <th>Nodes_number_rf_EC</th>\n",
       "      <th>Nodes_number_rf_Map</th>\n",
       "      <th>Nodes_number_rf_Mapcount</th>\n",
       "      <th>Nodes_present_rf_EC</th>\n",
       "      <th>Nodes_present_rf_Map</th>\n",
       "      <th>Nodes_present_rf_Mapcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n1</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n2</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n3</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n4</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n5</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Nodes_Name  Nodes_number_rf_EC  Nodes_number_rf_Map  \\\n",
       "0         n1            0.000080             0.000107   \n",
       "1         n2            0.000085             0.000094   \n",
       "2         n3            0.000312             0.000277   \n",
       "3         n4            0.000124             0.000101   \n",
       "4         n5            0.000063             0.000068   \n",
       "\n",
       "   Nodes_number_rf_Mapcount  Nodes_present_rf_EC  Nodes_present_rf_Map  \\\n",
       "0                  0.000107             0.000144              0.000163   \n",
       "1                  0.000094             0.000243              0.000224   \n",
       "2                  0.000277             0.000253              0.000123   \n",
       "3                  0.000101             0.000103              0.000137   \n",
       "4                  0.000068             0.000069              0.000074   \n",
       "\n",
       "   Nodes_present_rf_Mapcount  \n",
       "0                   0.000163  \n",
       "1                   0.000224  \n",
       "2                   0.000123  \n",
       "3                   0.000137  \n",
       "4                   0.000074  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Importance_data_Nodes = {'Nodes_Name': Nodes_present_features_names,\n",
    "                         'Nodes_number_rf_EC': Nodes_number_rf.feature_importances_,\n",
    "                         'Nodes_number_rf_Map': Nodes_number_rfmap.feature_importances_,\n",
    "                         'Nodes_number_rf_Mapcount': Nodes_number_rfmapcount.feature_importances_,\n",
    "                         'Nodes_present_rf_EC': Nodes_present_rf.feature_importances_,\n",
    "                         'Nodes_present_rf_Map': Nodes_present_rfmap.feature_importances_,\n",
    "                         'Nodes_present_rf_Mapcount': Nodes_present_rfmapcount.feature_importances_}\n",
    "Importance_data_Nodes_df = pd.DataFrame(Importance_data_Nodes)\n",
    "Importance_data_Nodes_df.to_csv('/home/robbenm/LuberLab/crispri/EcoBisTest/Nodes_importance.csv')\n",
    "Importance_data_Nodes_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aefec84",
   "metadata": {},
   "source": [
    "After loss calculations, lets also try giving our models to the adaboost (boosting) models to see if they improve accuracy at all. Let's just give it a wack with the motif numbers model set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fa982612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robbenm/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.02888481449525453"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(n_estimators = 100)\n",
    "scores = cross_val_score(clf, Motif_number_features_array, Motif_number_targetMap,cv = 5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482e2c2d",
   "metadata": {},
   "source": [
    "Lets run a random search to find favorable hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1d50940e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'SAMME.R',\n",
      " 'base_estimator': None,\n",
      " 'learning_rate': 1.0,\n",
      " 'n_estimators': 100,\n",
      " 'random_state': None}\n"
     ]
    }
   ],
   "source": [
    "pprint(clf.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572406bd",
   "metadata": {},
   "source": [
    "We can change the learning rate (0-1), n_estimators (100:10000), and base estimator (decision tree max_depth 1-10). Lets do a random CV to evaluate these parameters on at least the motif_number model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4a1e95fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': [0.1, 0.2, 0.4, 0.6, 0.8, 1, 1.25, 1.5, 2],\n",
      " 'n_estimators': [100, 500, 1000, 2500, 5000, 7500, 10000]}\n"
     ]
    }
   ],
   "source": [
    "n_estimators =  [100, 500, 1000, 2500, 5000, 7500, 10000] # num of trees\n",
    "learning_rate = [0.1,0.2,0.4,0.6,0.8,1,1.25,1.5,2]\n",
    "base_est = [DecisionTreeClassifier(max_depth=1),DecisionTreeClassifier(max_depth=3),DecisionTreeClassifier(max_depth=6),DecisionTreeClassifier(max_depth=9)]\n",
    "#Might not be able to run base_estimator max depth in randomfoldcv\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "              'learning_rate': learning_rate}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783d8cc8",
   "metadata": {},
   "source": [
    "Now lets run the randomfoldcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "159f5686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ada = AdaBoostClassifier()\n",
    "# Motif_present_ada = RandomizedSearchCV(estimator=ada, \n",
    "#                                param_distributions=random_grid,\n",
    "#                                n_iter=100, \n",
    "#                                cv = 5, \n",
    "#                                verbose =2, \n",
    "#                                random_state=42,\n",
    "#                                n_jobs = 16)\n",
    "# Motif_present_ada.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cf5403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motif_present_ada.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51df32a8",
   "metadata": {},
   "source": [
    "Looks like the best parameters for the adaboosted is using 7500 ntrees and a learning rate of 0.4. We will apply that to a series of models using different max depths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3e30b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1 = DecisionTreeClassifier(max_depth=1)\n",
    "dt3 = DecisionTreeClassifier(max_depth=3)\n",
    "dt6 = DecisionTreeClassifier(max_depth=6)\n",
    "dt9 = DecisionTreeClassifier(max_depth=9)\n",
    "dt12 = DecisionTreeClassifier(max_depth=12)\n",
    "ada1 = AdaBoostClassifier(base_estimator=dt1,learning_rate=0.4,n_estimators = 7500)\n",
    "ada3 = AdaBoostClassifier(base_estimator=dt3,learning_rate=0.4,n_estimators = 7500)\n",
    "ada6 = AdaBoostClassifier(base_estimator=dt6,learning_rate=0.4,n_estimators = 7500)\n",
    "ada9 = AdaBoostClassifier(base_estimator=dt9,learning_rate=0.4,n_estimators = 7500)\n",
    "ada12 = AdaBoostClassifier(base_estimator=dt12,learning_rate=0.4,n_estimators = 7500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "98ed7bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robbenm/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00985332182916307"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores = cross_val_score(ada1, Motif_number_features_array, Motif_number_targetEC,cv = 5,n_jobs = 5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6b6c9fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robbenm/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.01771786022433132"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores = cross_val_score(ada3, Motif_number_features_array, Motif_number_targetEC,cv = 5,n_jobs = 5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b113bc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robbenm/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/home/robbenm/.local/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.019687230371009494"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores = cross_val_score(ada6, Motif_number_features_array, Motif_number_targetEC,cv = 5,n_jobs = 5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a268f8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robbenm/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.02296807592752373"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores = cross_val_score(ada9, Motif_number_features_array, Motif_number_targetEC,cv = 5,n_jobs = 5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5ea1cdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robbenm/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.02230802415875755"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores = cross_val_score(ada12, Motif_number_features_array, Motif_number_targetEC,cv = 5,n_jobs = 5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c8968d",
   "metadata": {},
   "source": [
    "It looks like 9 wins, still a low score but whatevs let's make a model function and then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "42ac250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ada(features,labels,ncores = 1):\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(features,\n",
    "                                                                                labels,\n",
    "                                                                                test_size = 0.25, \n",
    "                                                                                random_state = 42)\n",
    "    #Make the random forest\n",
    "    ada = AdaBoostClassifier(base_estimator=dt9,learning_rate=0.4,n_estimators = 7500)\n",
    "#     cv_score = cross_val_score(rf, features, labels,cv = 5,n_jobs = 16)\n",
    "    ada.fit(train_features,train_labels)\n",
    "    \n",
    "    \n",
    "    print('Training score: ', ada.score(train_features, train_labels)*100)\n",
    "    print('Test score: ', ada.score(test_features, test_labels)*100)\n",
    "    \n",
    "    predictions = ada.predict(test_features)\n",
    "    y_pred = ada.predict_proba(test_features)\n",
    "    lloss = log_loss(test_labels,y_pred,labels=ada.classes_)\n",
    "    print('Log Loss: ',lloss)\n",
    "#     print('Cross validation score: ',cv_score.mean())\n",
    "    return(ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "78d9e1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  93.34500875656742\n",
      "Test score:  1.574803149606299\n",
      "Log Loss:  6.649269666903975\n"
     ]
    }
   ],
   "source": [
    "Motif_present_ada = make_ada(Motif_present_features_array,Motif_present_targetEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6f1f870c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  93.95796847635727\n",
      "Test score:  1.0498687664041995\n",
      "Log Loss:  6.883513747390988\n"
     ]
    }
   ],
   "source": [
    "Motif_number_ada = make_ada(Motif_number_features_array,Motif_number_targetEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "639c156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  58.90295358649789\n",
      "Test score:  8.080808080808081\n",
      "Log Loss:  4.212237406005534\n"
     ]
    }
   ],
   "source": [
    "Nodes_present_ada = make_ada(Nodes_present_features_array,Nodes_present_targetEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "21c52fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  56.87763713080168\n",
      "Test score:  7.828282828282829\n",
      "Log Loss:  4.234130539692141\n"
     ]
    }
   ],
   "source": [
    "Nodes_number_ada = make_ada(Nodes_number_features_array,Nodes_number_targetEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "68df0143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  89.14185639229422\n",
      "Test score:  2.6246719160104988\n",
      "Log Loss:  7.087755014384876\n"
     ]
    }
   ],
   "source": [
    "Motif_present_ada = make_ada(Motif_present_features_array,Motif_present_targetMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c843cd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  90.19264448336253\n",
      "Test score:  1.0498687664041995\n",
      "Log Loss:  7.276042964658349\n"
     ]
    }
   ],
   "source": [
    "Motif_number_ada = make_ada(Motif_number_features_array,Motif_number_targetMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c28639c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  23.375527426160335\n",
      "Test score:  6.8181818181818175\n",
      "Log Loss:  5.18175028335019\n"
     ]
    }
   ],
   "source": [
    "Nodes_present_ada = make_ada(Nodes_present_features_array,Nodes_present_targetMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "facb21d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  23.20675105485232\n",
      "Test score:  6.313131313131313\n",
      "Log Loss:  5.1966185575841\n"
     ]
    }
   ],
   "source": [
    "Nodes_number_ada = make_ada(Nodes_number_features_array,Nodes_number_targetMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1dc673ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_ada(features,labels,ncores = 1):\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(features,\n",
    "                                                                                labels,\n",
    "                                                                                test_size = 0.25, \n",
    "                                                                                random_state = 42)\n",
    "    #Make the random forest\n",
    "    ada = AdaBoostClassifier(base_estimator=dt9,learning_rate=0.4,n_estimators = 7500)\n",
    "#     cv_score = cross_val_score(rf, features, labels,cv = 5,n_jobs = 16)\n",
    "    ada.fit(train_features,train_labels)\n",
    "    predictions = ada.predict(test_features)\n",
    "    errors = 0\n",
    "    length = 0\n",
    "    less = 0\n",
    "    equal = 0\n",
    "    more = 0\n",
    "    for x in range(0,len(predictions)):\n",
    "        map1 = predictions[x]#['path:map00280', 'path:map00630']\n",
    "        map2 = test_labels[x]\n",
    "        map1 = map1.replace('[','').replace(']','').split(',')\n",
    "        map2 = map2.replace('[','').replace(']','').split(',')\n",
    "        total = len(set(map1).intersection(map2))\n",
    "        errors = errors + total\n",
    "        length = length + len(map2)\n",
    "        if (total < len(map2)): less = less + 1\n",
    "        if (total == len(map2)): equal = equal + 1\n",
    "        if (total < len(map1)): more = more + 1\n",
    "    print('accuracy was ',errors/length*100,'%')\n",
    "    print('in ', less/len(test_labels)*100,'% of genes, model did not predict all pathways')\n",
    "    print('in ', equal/len(test_labels)*100,'% of genes, model predicted all pathways')\n",
    "    print('in ', more/len(test_labels)*100,'% of genes, model predicted pathways not found in gene')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b3df10bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy was  30.093312597200622 %\n",
      "in  96.8503937007874 % of genes, model did not predict all pathways\n",
      "in  3.149606299212598 % of genes, model predicted all pathways\n",
      "in  96.06299212598425 % of genes, model predicted pathways not found in gene\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=9),\n",
       "                   learning_rate=0.4, n_estimators=7500)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_ada(Motif_present_features_array,Motif_present_targetMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3b246ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy was  29.307568438003223 %\n",
      "in  98.9501312335958 % of genes, model did not predict all pathways\n",
      "in  1.0498687664041995 % of genes, model predicted all pathways\n",
      "in  97.9002624671916 % of genes, model predicted pathways not found in gene\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=9),\n",
       "                   learning_rate=0.4, n_estimators=7500)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_ada(Motif_number_features_array,Motif_number_targetMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cc41db3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy was  7.742878013148284 %\n",
      "in  93.18181818181817 % of genes, model did not predict all pathways\n",
      "in  6.8181818181818175 % of genes, model predicted all pathways\n",
      "in  93.18181818181817 % of genes, model predicted pathways not found in gene\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=9),\n",
       "                   learning_rate=0.4, n_estimators=7500)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_ada(Nodes_present_features_array,Nodes_present_targetMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d0f7af0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy was  7.377647918188458 %\n",
      "in  93.68686868686868 % of genes, model did not predict all pathways\n",
      "in  6.313131313131313 % of genes, model predicted all pathways\n",
      "in  93.68686868686868 % of genes, model predicted pathways not found in gene\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=9),\n",
       "                   learning_rate=0.4, n_estimators=7500)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_ada(Nodes_number_features_array,Nodes_number_targetMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a9e455",
   "metadata": {},
   "source": [
    "Let's do the Gradient Boost now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "51fd124e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'categorical_features': None,\n",
      " 'early_stopping': 'auto',\n",
      " 'l2_regularization': 0.0,\n",
      " 'learning_rate': 0.1,\n",
      " 'loss': 'auto',\n",
      " 'max_bins': 255,\n",
      " 'max_depth': None,\n",
      " 'max_iter': 100,\n",
      " 'max_leaf_nodes': 31,\n",
      " 'min_samples_leaf': 20,\n",
      " 'monotonic_cst': None,\n",
      " 'n_iter_no_change': 10,\n",
      " 'random_state': None,\n",
      " 'scoring': 'loss',\n",
      " 'tol': 1e-07,\n",
      " 'validation_fraction': 0.1,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "clf = HistGradientBoostingClassifier()\n",
    "pprint(clf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c8e7fb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robbenm/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=2; total time=   0.1s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=2; total time=   0.0s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=2; total time=   0.0s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=2; total time=   0.0s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=100, max_leaf_nodes=40, min_samples_leaf=30; total time=   0.0s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=100, max_leaf_nodes=40, min_samples_leaf=30; total time=   0.0s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=100, max_leaf_nodes=40, min_samples_leaf=30; total time=   0.0s\n",
      "[CV] END learning_rate=0.4, max_depth=1, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=10; total time= 7.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robbenm/.local/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=1.2, max_depth=1, max_iter=100, max_leaf_nodes=20, min_samples_leaf=2; total time=   9.3s\n",
      "[CV] END learning_rate=0.4, max_depth=1, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=10; total time= 7.4min\n",
      "[CV] END learning_rate=0.1, max_depth=5, max_iter=100, max_leaf_nodes=20, min_samples_leaf=10; total time= 1.0min\n",
      "[CV] END learning_rate=0.8, max_depth=1, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=10; total time= 7.4min\n",
      "[CV] END learning_rate=0.1, max_depth=5, max_iter=100, max_leaf_nodes=20, min_samples_leaf=10; total time= 1.0min\n",
      "[CV] END learning_rate=0.8, max_depth=1, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=10; total time= 7.4min\n",
      "[CV] END learning_rate=1.2, max_depth=5, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=20; total time= 1.5min\n",
      "[CV] END learning_rate=0.8, max_depth=1, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=10; total time= 7.3min\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=2; total time=   0.1s\n",
      "[CV] END learning_rate=1.2, max_depth=1, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=30; total time=14.7min\n",
      "[CV] END learning_rate=1.2, max_depth=1, max_iter=100, max_leaf_nodes=20, min_samples_leaf=2; total time=   9.5s\n",
      "[CV] END learning_rate=1.2, max_depth=10, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=2; total time=14.6min\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=2; total time=   0.0s\n",
      "[CV] END learning_rate=1.2, max_depth=1, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=30; total time=15.1min\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=100, max_leaf_nodes=20, min_samples_leaf=2; total time=  10.7s\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=2; total time= 1.5min\n",
      "[CV] END learning_rate=1.2, max_depth=1, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=2; total time= 1.3min\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=2; total time=   0.0s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=2; total time=   0.0s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=2; total time=   0.0s\n",
      "[CV] END learning_rate=0.4, max_depth=1, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=10; total time=13.7min\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=100, max_leaf_nodes=20, min_samples_leaf=2; total time=  11.1s\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=2; total time= 1.5min\n",
      "[CV] END learning_rate=0.8, max_depth=10, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=2; total time= 1.5min\n",
      "[CV] END learning_rate=0.4, max_depth=1, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=10; total time=15.3min\n",
      "[CV] END learning_rate=1.2, max_depth=5, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=20; total time= 1.5min\n",
      "[CV] END learning_rate=1.2, max_depth=1, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=2; total time= 1.6min\n",
      "[CV] END learning_rate=0.4, max_depth=1, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=10; total time=16.0min\n",
      "[CV] END learning_rate=0.1, max_depth=10, max_iter=100, max_leaf_nodes=40, min_samples_leaf=2; total time=  11.8s\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=2; total time= 1.6min\n",
      "[CV] END learning_rate=0.8, max_depth=10, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=2; total time= 1.6min\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=10; total time=   0.1s\n",
      "[CV] END learning_rate=1.2, max_depth=None, max_iter=100, max_leaf_nodes=40, min_samples_leaf=10; total time=   0.0s\n",
      "[CV] END learning_rate=1.2, max_depth=None, max_iter=100, max_leaf_nodes=40, min_samples_leaf=10; total time=   0.0s\n",
      "[CV] END learning_rate=1.2, max_depth=1, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=30; total time= 1.5min\n",
      "[CV] END learning_rate=0.4, max_depth=5, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=2; total time=15.6min\n",
      "[CV] END learning_rate=0.1, max_depth=10, max_iter=100, max_leaf_nodes=40, min_samples_leaf=2; total time=  18.6s\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=10; total time= 1.5min\n",
      "[CV] END learning_rate=0.8, max_depth=10, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=2; total time= 1.7min\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=10; total time=   0.1s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=10; total time=   0.0s\n",
      "[CV] END learning_rate=1.2, max_depth=None, max_iter=100, max_leaf_nodes=40, min_samples_leaf=10; total time=   0.1s\n",
      "[CV] END learning_rate=1.2, max_depth=1, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=30; total time= 1.5min\n",
      "[CV] END learning_rate=1.2, max_depth=1, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=30; total time= 1.5min\n",
      "[CV] END learning_rate=0.4, max_depth=5, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=2; total time=15.0min\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=100, max_leaf_nodes=20, min_samples_leaf=2; total time=   9.8s\n",
      "[CV] END learning_rate=1.2, max_depth=10, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=2; total time=25.5min\n",
      "[CV] END learning_rate=1.2, max_depth=1, max_iter=100, max_leaf_nodes=20, min_samples_leaf=2; total time=   9.8s\n",
      "[CV] END learning_rate=1.2, max_depth=10, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=2; total time=29.2min\n",
      "[CV] END learning_rate=0.4, max_depth=5, max_iter=100, max_leaf_nodes=60, min_samples_leaf=30; total time=   9.9s\n",
      "[CV] END learning_rate=0.4, max_depth=5, max_iter=100, max_leaf_nodes=60, min_samples_leaf=30; total time=   9.9s\n",
      "[CV] END learning_rate=0.4, max_depth=5, max_iter=100, max_leaf_nodes=60, min_samples_leaf=30; total time=  10.4s\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=100, max_leaf_nodes=40, min_samples_leaf=30; total time=   9.8s\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=100, max_leaf_nodes=40, min_samples_leaf=30; total time=   9.6s\n",
      "[CV] END learning_rate=1.2, max_depth=1, max_iter=100, max_leaf_nodes=40, min_samples_leaf=10; total time=   8.7s\n",
      "[CV] END learning_rate=1.2, max_depth=1, max_iter=100, max_leaf_nodes=40, min_samples_leaf=10; total time=   9.4s\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=2; total time=13.8min\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=30; total time= 8.7min\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=100, max_leaf_nodes=40, min_samples_leaf=30; total time=  10.0s\n",
      "[CV] END learning_rate=1.2, max_depth=1, max_iter=100, max_leaf_nodes=40, min_samples_leaf=10; total time=   9.3s\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=2; total time=14.8min\n",
      "[CV] END learning_rate=0.4, max_depth=1, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=20; total time= 1.5min\n",
      "[CV] END learning_rate=0.4, max_depth=1, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=20; total time= 1.5min\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=100, max_leaf_nodes=40, min_samples_leaf=20; total time=  29.6s\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=5000, max_leaf_nodes=60, min_samples_leaf=2; total time= 7.9min\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=100, max_leaf_nodes=20, min_samples_leaf=20; total time=  12.3s\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=20; total time=22.2min\n",
      "[CV] END learning_rate=0.4, max_depth=5, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=2; total time= 1.5min\n",
      "[CV] END learning_rate=0.1, max_depth=None, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=20; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=None, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=20; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=None, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=20; total time=   0.0s\n",
      "[CV] END learning_rate=1.2, max_depth=1, max_iter=100, max_leaf_nodes=60, min_samples_leaf=30; total time=   9.1s\n",
      "[CV] END learning_rate=1.2, max_depth=1, max_iter=100, max_leaf_nodes=60, min_samples_leaf=30; total time=   9.3s\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=10; total time=20.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.1, max_depth=5, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=2; total time=14.8min\n",
      "[CV] END learning_rate=1.2, max_depth=10, max_iter=10000, max_leaf_nodes=60, min_samples_leaf=2; total time=16.0min\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=20; total time= 1.5min\n",
      "[CV] END learning_rate=1.2, max_depth=5, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=2; total time= 1.6min\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=30; total time=16.3min\n",
      "[CV] END learning_rate=0.1, max_depth=10, max_iter=100, max_leaf_nodes=40, min_samples_leaf=2; total time=  19.3s\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=10; total time= 1.5min\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=20; total time=42.2min\n",
      "[CV] END learning_rate=1.2, max_depth=5, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=20; total time= 1.5min\n",
      "[CV] END learning_rate=1.2, max_depth=None, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=2; total time=   0.0s\n",
      "[CV] END learning_rate=1.2, max_depth=None, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=2; total time=   0.1s\n",
      "[CV] END learning_rate=1.2, max_depth=None, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=2; total time=   0.0s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=10; total time=   0.0s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=10; total time=   0.1s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=10; total time=   0.0s\n",
      "[CV] END learning_rate=1.2, max_depth=1, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=2; total time= 1.5min\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=20; total time=41.7min\n",
      "[CV] END learning_rate=0.1, max_depth=5, max_iter=100, max_leaf_nodes=20, min_samples_leaf=10; total time=  24.9s\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=10; total time= 1.5min\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=20; total time=44.2min\n",
      "[CV] END learning_rate=0.4, max_depth=1, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=10; total time= 7.4min\n",
      "[CV] END learning_rate=0.8, max_depth=None, max_iter=10000, max_leaf_nodes=60, min_samples_leaf=30; total time=   0.0s\n",
      "[CV] END learning_rate=0.8, max_depth=None, max_iter=10000, max_leaf_nodes=60, min_samples_leaf=30; total time=   0.0s\n",
      "[CV] END learning_rate=0.8, max_depth=None, max_iter=10000, max_leaf_nodes=60, min_samples_leaf=30; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=2; total time=14.8min\n",
      "[CV] END learning_rate=1.2, max_depth=5, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=2; total time= 1.5min\n",
      "[CV] END learning_rate=1.2, max_depth=10, max_iter=10000, max_leaf_nodes=60, min_samples_leaf=2; total time=26.2min\n",
      "[CV] END learning_rate=0.4, max_depth=5, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=10; total time= 1.4min\n",
      "[CV] END learning_rate=0.1, max_depth=5, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=10; total time=13.2min\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=20; total time=21.7min\n",
      "[CV] END learning_rate=0.1, max_depth=10, max_iter=100, max_leaf_nodes=60, min_samples_leaf=30; total time= 1.4min\n",
      "[CV] END learning_rate=0.4, max_depth=5, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=30; total time=14.4min\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=30; total time=15.2min\n",
      "[CV] END learning_rate=0.1, max_depth=10, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=10; total time=13.6min\n",
      "[CV] END learning_rate=1.2, max_depth=1, max_iter=10000, max_leaf_nodes=60, min_samples_leaf=2; total time=15.6min\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=100, max_leaf_nodes=20, min_samples_leaf=20; total time=   9.2s\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=100, max_leaf_nodes=20, min_samples_leaf=20; total time=   9.7s\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=100, max_leaf_nodes=20, min_samples_leaf=20; total time=   9.5s\n",
      "[CV] END learning_rate=0.8, max_depth=1, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=2; total time= 7.1min\n",
      "[CV] END learning_rate=1.2, max_depth=10, max_iter=10000, max_leaf_nodes=60, min_samples_leaf=2; total time=25.0min\n",
      "[CV] END learning_rate=0.8, max_depth=10, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=2; total time=14.8min\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=100, max_leaf_nodes=20, min_samples_leaf=20; total time=   9.4s\n",
      "[CV] END learning_rate=1.2, max_depth=None, max_iter=100, max_leaf_nodes=40, min_samples_leaf=20; total time=   0.0s\n",
      "[CV] END learning_rate=1.2, max_depth=None, max_iter=100, max_leaf_nodes=40, min_samples_leaf=20; total time=   0.0s\n",
      "[CV] END learning_rate=1.2, max_depth=None, max_iter=100, max_leaf_nodes=40, min_samples_leaf=20; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=20; total time=21.3min\n",
      "[CV] END learning_rate=1.2, max_depth=10, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=2; total time=12.6min\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=30; total time= 7.6min\n",
      "[CV] END learning_rate=0.8, max_depth=1, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=2; total time= 6.9min\n",
      "[CV] END learning_rate=1.2, max_depth=10, max_iter=5000, max_leaf_nodes=60, min_samples_leaf=20; total time= 6.8min\n",
      "[CV] END learning_rate=1.2, max_depth=None, max_iter=100, max_leaf_nodes=60, min_samples_leaf=30; total time=   0.0s\n",
      "[CV] END learning_rate=1.2, max_depth=None, max_iter=100, max_leaf_nodes=60, min_samples_leaf=30; total time=   0.0s\n",
      "[CV] END learning_rate=1.2, max_depth=None, max_iter=100, max_leaf_nodes=60, min_samples_leaf=30; total time=   0.0s\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=20; total time= 9.3min\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=20; total time= 1.4min\n",
      "[CV] END learning_rate=0.8, max_depth=1, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=2; total time=13.9min\n",
      "[CV] END learning_rate=0.4, max_depth=5, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=2; total time=19.4min\n",
      "[CV] END learning_rate=0.8, max_depth=1, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=30; total time= 1.6min\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=10; total time= 8.0min\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=5000, max_leaf_nodes=60, min_samples_leaf=2; total time= 7.3min\n",
      "[CV] END learning_rate=1.2, max_depth=10, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=2; total time=13.2min\n",
      "[CV] END learning_rate=0.1, max_depth=5, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=2; total time=14.2min\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=20; total time= 1.3min\n",
      "[CV] END learning_rate=1.2, max_depth=10, max_iter=5000, max_leaf_nodes=60, min_samples_leaf=20; total time= 6.3min\n",
      "[CV] END learning_rate=0.1, max_depth=5, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=10; total time= 1.6min\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=20; total time= 6.5min\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=10; total time= 4.3min\n",
      "[CV] END learning_rate=0.8, max_depth=1, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=20; total time=12.9min\n",
      "[CV] END learning_rate=0.1, max_depth=5, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=10; total time= 7.1min\n",
      "[CV] END learning_rate=0.1, max_depth=10, max_iter=100, max_leaf_nodes=60, min_samples_leaf=30; total time= 1.4min\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=10; total time= 4.7min\n",
      "[CV] END learning_rate=0.8, max_depth=1, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=20; total time=12.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.1, max_depth=None, max_iter=100, max_leaf_nodes=60, min_samples_leaf=20; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=None, max_iter=100, max_leaf_nodes=60, min_samples_leaf=20; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=None, max_iter=100, max_leaf_nodes=60, min_samples_leaf=20; total time=   0.1s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=100, max_leaf_nodes=20, min_samples_leaf=2; total time=   0.0s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=100, max_leaf_nodes=20, min_samples_leaf=2; total time=   0.1s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=100, max_leaf_nodes=20, min_samples_leaf=2; total time=   0.1s\n",
      "[CV] END learning_rate=0.8, max_depth=1, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=2; total time= 1.5min\n",
      "[CV] END learning_rate=0.1, max_depth=10, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=20; total time=22.7min\n",
      "[CV] END learning_rate=0.4, max_depth=5, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=2; total time=14.8min\n",
      "[CV] END learning_rate=1.2, max_depth=None, max_iter=100, max_leaf_nodes=20, min_samples_leaf=30; total time=   0.0s\n",
      "[CV] END learning_rate=1.2, max_depth=None, max_iter=100, max_leaf_nodes=20, min_samples_leaf=30; total time=   0.0s\n",
      "[CV] END learning_rate=1.2, max_depth=None, max_iter=100, max_leaf_nodes=20, min_samples_leaf=30; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=20; total time= 6.0min\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=20; total time=   0.1s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=20; total time=   0.0s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=20; total time=   0.0s\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=10; total time= 7.7min\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=100, max_leaf_nodes=40, min_samples_leaf=20; total time=  30.6s\n",
      "[CV] END learning_rate=1.2, max_depth=10, max_iter=100, max_leaf_nodes=20, min_samples_leaf=2; total time=  16.3s\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=5000, max_leaf_nodes=60, min_samples_leaf=2; total time= 7.5min\n",
      "[CV] END learning_rate=0.1, max_depth=10, max_iter=100, max_leaf_nodes=60, min_samples_leaf=30; total time= 1.4min\n",
      "[CV] END learning_rate=0.4, max_depth=5, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=30; total time=13.7min\n",
      "[CV] END learning_rate=1.2, max_depth=10, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=20; total time= 1.9min\n",
      "[CV] END learning_rate=0.1, max_depth=10, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=20; total time=24.3min\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=2; total time=13.4min\n",
      "[CV] END learning_rate=1.2, max_depth=10, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=20; total time= 1.3min\n",
      "[CV] END learning_rate=0.8, max_depth=1, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=2; total time= 1.4min\n",
      "[CV] END learning_rate=1.2, max_depth=10, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=2; total time= 6.8min\n",
      "[CV] END learning_rate=0.8, max_depth=None, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=10; total time=   0.0s\n",
      "[CV] END learning_rate=0.8, max_depth=None, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=10; total time=   0.0s\n",
      "[CV] END learning_rate=0.8, max_depth=None, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=10; total time=   0.0s\n",
      "[CV] END learning_rate=0.8, max_depth=1, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=20; total time=11.8min\n",
      "[CV] END learning_rate=0.8, max_depth=1, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=10; total time=14.5min\n",
      "[CV] END learning_rate=0.8, max_depth=None, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=10; total time=   0.0s\n",
      "[CV] END learning_rate=0.8, max_depth=None, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=10; total time=   0.0s\n",
      "[CV] END learning_rate=0.8, max_depth=None, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=10; total time=   0.0s\n",
      "[CV] END learning_rate=0.4, max_depth=5, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=2; total time= 1.8min\n",
      "[CV] END learning_rate=1.2, max_depth=1, max_iter=100, max_leaf_nodes=60, min_samples_leaf=30; total time=   9.7s\n",
      "[CV] END learning_rate=0.1, max_depth=None, max_iter=5000, max_leaf_nodes=60, min_samples_leaf=10; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=None, max_iter=5000, max_leaf_nodes=60, min_samples_leaf=10; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=None, max_iter=5000, max_leaf_nodes=60, min_samples_leaf=10; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=None, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=30; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=None, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=30; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=None, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=30; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=None, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=30; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=None, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=30; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=None, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=30; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=10; total time=21.9min\n",
      "[CV] END learning_rate=0.4, max_depth=5, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=30; total time=15.7min\n",
      "[CV] END learning_rate=0.4, max_depth=5, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=10; total time= 1.5min\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=20; total time= 1.7min\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=5000, max_leaf_nodes=60, min_samples_leaf=10; total time=19.2min\n",
      "[CV] END learning_rate=0.4, max_depth=5, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=2; total time=16.7min\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=30; total time=14.7min\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=20; total time= 1.7min\n",
      "[CV] END learning_rate=0.8, max_depth=1, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=2; total time=14.5min\n",
      "[CV] END learning_rate=0.8, max_depth=1, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=2; total time=14.3min\n",
      "[CV] END learning_rate=1.2, max_depth=1, max_iter=10000, max_leaf_nodes=60, min_samples_leaf=2; total time=14.7min\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=100, max_leaf_nodes=20, min_samples_leaf=20; total time=  11.3s\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=20; total time= 3.7min\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=30; total time=12.9min\n",
      "[CV] END learning_rate=1.2, max_depth=10, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=20; total time=  58.3s\n",
      "[CV] END learning_rate=0.8, max_depth=None, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=2; total time=   0.0s\n",
      "[CV] END learning_rate=0.8, max_depth=None, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=2; total time=   0.0s\n",
      "[CV] END learning_rate=0.8, max_depth=None, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=2; total time=   0.0s\n",
      "[CV] END learning_rate=0.8, max_depth=10, max_iter=100, max_leaf_nodes=20, min_samples_leaf=2; total time=   6.3s\n",
      "[CV] END learning_rate=0.8, max_depth=10, max_iter=100, max_leaf_nodes=20, min_samples_leaf=2; total time=   7.2s\n",
      "[CV] END learning_rate=0.8, max_depth=10, max_iter=100, max_leaf_nodes=20, min_samples_leaf=2; total time=   7.1s\n",
      "[CV] END learning_rate=0.8, max_depth=1, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=2; total time=  57.8s\n",
      "[CV] END learning_rate=0.1, max_depth=10, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=20; total time=22.9min\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=5000, max_leaf_nodes=60, min_samples_leaf=10; total time=19.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=1.2, max_depth=1, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=30; total time=14.2min\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=30; total time= 7.2min\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=20; total time= 1.4min\n",
      "[CV] END learning_rate=1.2, max_depth=5, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=2; total time= 2.4min\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=30; total time=14.6min\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=5000, max_leaf_nodes=60, min_samples_leaf=10; total time=20.2min\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=30; total time= 7.3min\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=100, max_leaf_nodes=20, min_samples_leaf=30; total time=   9.0s\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=100, max_leaf_nodes=20, min_samples_leaf=30; total time=   9.9s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=30; total time=   0.1s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=30; total time=   0.1s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=30; total time=   0.1s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=20; total time=   0.0s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=20; total time=   0.0s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=20; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=2; total time=38.8min\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=2; total time=18.4min\n",
      "[CV] END learning_rate=1.2, max_depth=1, max_iter=10000, max_leaf_nodes=60, min_samples_leaf=2; total time=14.8min\n",
      "[CV] END learning_rate=0.4, max_depth=5, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=2; total time= 1.5min\n",
      "[CV] END learning_rate=0.8, max_depth=1, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=2; total time= 6.7min\n",
      "[CV] END learning_rate=0.8, max_depth=None, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=2; total time=   0.0s\n",
      "[CV] END learning_rate=0.8, max_depth=None, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=2; total time=   0.1s\n",
      "[CV] END learning_rate=0.8, max_depth=None, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=2; total time=   0.0s\n",
      "[CV] END learning_rate=1.2, max_depth=10, max_iter=5000, max_leaf_nodes=60, min_samples_leaf=20; total time= 7.3min\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=20; total time= 8.0min\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=10; total time= 4.7min\n",
      "[CV] END learning_rate=0.4, max_depth=1, max_iter=100, max_leaf_nodes=60, min_samples_leaf=20; total time=   9.8s\n",
      "[CV] END learning_rate=0.4, max_depth=1, max_iter=100, max_leaf_nodes=60, min_samples_leaf=20; total time=   9.3s\n",
      "[CV] END learning_rate=0.4, max_depth=1, max_iter=100, max_leaf_nodes=60, min_samples_leaf=20; total time=  10.0s\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=100, max_leaf_nodes=60, min_samples_leaf=20; total time=   9.9s\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=100, max_leaf_nodes=60, min_samples_leaf=20; total time=   9.9s\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=100, max_leaf_nodes=60, min_samples_leaf=20; total time=  10.0s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=2; total time=   0.1s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=2; total time=   0.0s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=2; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=2; total time=18.8min\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=2; total time=18.2min\n",
      "[CV] END learning_rate=0.4, max_depth=5, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=2; total time=14.7min\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=30; total time= 4.5min\n",
      "[CV] END learning_rate=0.8, max_depth=1, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=30; total time= 1.6min\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=10; total time= 7.4min\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=2; total time=36.6min\n",
      "[CV] END learning_rate=0.4, max_depth=5, max_iter=1000, max_leaf_nodes=60, min_samples_leaf=10; total time= 1.3min\n",
      "[CV] END learning_rate=0.1, max_depth=5, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=10; total time=33.6min\n",
      "[CV] END learning_rate=0.8, max_depth=1, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=10; total time=14.7min\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=30; total time= 4.6min\n",
      "[CV] END learning_rate=0.8, max_depth=1, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=30; total time= 1.4min\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=30; total time= 7.3min\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=100, max_leaf_nodes=20, min_samples_leaf=30; total time=   9.9s\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=2; total time=39.6min\n",
      "[CV] END learning_rate=0.8, max_depth=10, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=2; total time=14.9min\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=20; total time= 4.6min\n",
      "[CV] END learning_rate=0.8, max_depth=5, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=30; total time= 7.3min\n",
      "[CV] END learning_rate=0.4, max_depth=1, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=20; total time= 1.5min\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=10000, max_leaf_nodes=60, min_samples_leaf=30; total time=   0.0s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=10000, max_leaf_nodes=60, min_samples_leaf=30; total time=   0.0s\n",
      "[CV] END learning_rate=0.4, max_depth=None, max_iter=10000, max_leaf_nodes=60, min_samples_leaf=30; total time=   0.1s\n",
      "[CV] END learning_rate=0.8, max_depth=None, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=10; total time=   0.0s\n",
      "[CV] END learning_rate=0.8, max_depth=None, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=10; total time=   0.1s\n",
      "[CV] END learning_rate=0.8, max_depth=None, max_iter=5000, max_leaf_nodes=20, min_samples_leaf=10; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=100, max_leaf_nodes=40, min_samples_leaf=20; total time=  30.0s\n",
      "[CV] END learning_rate=1.2, max_depth=10, max_iter=100, max_leaf_nodes=20, min_samples_leaf=2; total time=  11.5s\n",
      "[CV] END learning_rate=1.2, max_depth=10, max_iter=100, max_leaf_nodes=20, min_samples_leaf=2; total time=  16.8s\n",
      "[CV] END learning_rate=0.1, max_depth=5, max_iter=1000, max_leaf_nodes=20, min_samples_leaf=10; total time= 5.3min\n",
      "[CV] END learning_rate=0.1, max_depth=10, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=10; total time=36.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robbenm/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "87 fits failed out of a total of 300.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "87 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/robbenm/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/robbenm/.local/lib/python3.8/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 510, in fit\n",
      "    grower = TreeGrower(\n",
      "  File \"/home/robbenm/.local/lib/python3.8/site-packages/sklearn/ensemble/_hist_gradient_boosting/grower.py\", line 207, in __init__\n",
      "    self._validate_parameters(\n",
      "  File \"/home/robbenm/.local/lib/python3.8/site-packages/sklearn/ensemble/_hist_gradient_boosting/grower.py\", line 334, in _validate_parameters\n",
      "    if max_depth is not None and max_depth < 1:\n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/robbenm/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.00525395 0.00875351 0.00525395 0.00612654 0.00438136        nan\n",
      " 0.02802183 0.00875121 0.00350647 0.01400976 0.00525395        nan\n",
      "        nan 0.00789013 0.01225538 0.0096261  0.0105079  0.00438136\n",
      "        nan        nan 0.00525395 0.00350647 0.02539025        nan\n",
      " 0.00789013        nan        nan 0.00525395 0.00875121        nan\n",
      " 0.01400516 0.00262467        nan 0.00787862 0.03064189 0.01400746\n",
      "        nan 0.02626514        nan 0.02539025 0.01489616 0.00438136\n",
      "        nan 0.00525395        nan        nan        nan 0.02714003\n",
      " 0.00437676 0.00963531 0.00525395 0.0105148  0.00262467 0.01138509\n",
      " 0.00963531        nan 0.00350186 0.01489616 0.01226459        nan\n",
      " 0.00612654 0.01225538 0.00963531 0.01751393 0.00963531        nan\n",
      "        nan 0.02801492        nan        nan 0.03765483 0.01225538\n",
      " 0.0105148  0.02889672        nan 0.00963301 0.00612654        nan\n",
      " 0.00613114        nan        nan 0.00438136 0.03063959        nan\n",
      " 0.0210204  0.01225538 0.03326426 0.0105125  0.02714003 0.00437676\n",
      " 0.02889672 0.0105079  0.02714003 0.00438136        nan 0.00438136\n",
      " 0.01751393 0.01489616        nan 0.02714003]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m Motif_present_hist \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(estimator\u001b[38;5;241m=\u001b[39mclf, \n\u001b[1;32m      8\u001b[0m                                param_distributions\u001b[38;5;241m=\u001b[39mrandom_grid,\n\u001b[1;32m      9\u001b[0m                                n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m                                random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m     13\u001b[0m                                n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m     14\u001b[0m Motif_present_hist\u001b[38;5;241m.\u001b[39mfit(train_features, train_labels)\n\u001b[0;32m---> 15\u001b[0m pprint(\u001b[43mMotif_present_ada\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_params_\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_params_'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.8, max_depth=1, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=10; total time=14.7min\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=1000, max_leaf_nodes=40, min_samples_leaf=30; total time= 4.4min\n",
      "[CV] END learning_rate=0.4, max_depth=10, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=30; total time=15.3min\n",
      "[CV] END learning_rate=0.8, max_depth=None, max_iter=10000, max_leaf_nodes=60, min_samples_leaf=2; total time=   0.0s\n",
      "[CV] END learning_rate=0.8, max_depth=None, max_iter=10000, max_leaf_nodes=60, min_samples_leaf=2; total time=   0.0s\n",
      "[CV] END learning_rate=0.8, max_depth=None, max_iter=10000, max_leaf_nodes=60, min_samples_leaf=2; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=10, max_iter=10000, max_leaf_nodes=20, min_samples_leaf=10; total time=41.3min\n",
      "[CV] END learning_rate=0.8, max_depth=10, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=2; total time=16.5min\n",
      "[CV] END learning_rate=0.1, max_depth=1, max_iter=5000, max_leaf_nodes=40, min_samples_leaf=10; total time=21.9min\n",
      "[CV] END learning_rate=0.1, max_depth=5, max_iter=10000, max_leaf_nodes=40, min_samples_leaf=10; total time=40.1min\n"
     ]
    }
   ],
   "source": [
    "random_grid = {'learning_rate': [0.1,0.4,0.8,1.2],\n",
    "              'max_depth': ['None',1,5,10],\n",
    "              'max_iter': [100,1000,5000,10000],\n",
    "              'max_leaf_nodes': [20,40,60],\n",
    "              'min_samples_leaf': [2,10,20,30]}\n",
    "clf = HistGradientBoostingClassifier()\n",
    "Motif_present_hist = RandomizedSearchCV(estimator=clf, \n",
    "                               param_distributions=random_grid,\n",
    "                               n_iter=100, \n",
    "                               cv = 3, \n",
    "                               verbose =2, \n",
    "                               random_state=42,\n",
    "                               n_jobs = 32)\n",
    "# Motif_present_hist.fit(train_features, train_labels)\n",
    "# pprint(Motif_present_hist.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "74b74933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1,\n",
      " 'max_depth': 1,\n",
      " 'max_iter': 100,\n",
      " 'max_leaf_nodes': 40,\n",
      " 'min_samples_leaf': 20}\n"
     ]
    }
   ],
   "source": [
    "pprint(Motif_present_hist.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd7e3c0",
   "metadata": {},
   "source": [
    "Looks like the best options for the gradient is a learning rate of 0.1, max depth 1, max iter 100, max_leaf_nodes 40, and min samples leaf 20. Now lets make it a function and run the calc for every model and predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c5a1888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hist(features,labels,ncores = 1):\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(features,\n",
    "                                                                                labels,\n",
    "                                                                                test_size = 0.25, \n",
    "                                                                                random_state = 42)\n",
    "    #Make the random forest\n",
    "    hist = HistGradientBoostingClassifier(learning_rate = 0.1, max_depth = 1, max_iter = 100, max_leaf_nodes = 40, min_samples_leaf = 20)\n",
    "#     cv_score = cross_val_score(rf, features, labels,cv = 5,n_jobs = 16)\n",
    "    hist.fit(train_features,train_labels)\n",
    "    \n",
    "    \n",
    "    print('Training score: ', hist.score(train_features, train_labels)*100)\n",
    "    print('Test score: ', hist.score(test_features, test_labels)*100)\n",
    "    \n",
    "    predictions = hist.predict(test_features)\n",
    "    y_pred = hist.predict_proba(test_features)\n",
    "    lloss = log_loss(test_labels,y_pred,labels=hist.classes_)\n",
    "    print('Log Loss: ',lloss)\n",
    "#     print('Cross validation score: ',cv_score.mean())\n",
    "    predictions = hist.predict(test_features)\n",
    "    \n",
    "    errors = 0\n",
    "    length = 0\n",
    "    less = 0\n",
    "    equal = 0\n",
    "    more = 0\n",
    "    if (ncores > 1):\n",
    "        for x in range(0,len(predictions)):\n",
    "            map1 = predictions[x]#['path:map00280', 'path:map00630']\n",
    "            map2 = test_labels[x]\n",
    "            map1 = map1.replace('[','').replace(']','').split(',')\n",
    "            map2 = map2.replace('[','').replace(']','').split(',')\n",
    "            total = len(set(map1).intersection(map2))\n",
    "            errors = errors + total\n",
    "            length = length + len(map2)\n",
    "            if (total < len(map2)): less = less + 1\n",
    "            if (total == len(map2)): equal = equal + 1\n",
    "            if (total < len(map1)): more = more + 1\n",
    "        print('Partial accuracy was ',errors/length*100,'%')\n",
    "    return(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9c1402e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  92.81961471103327\n",
      "Test score:  4.986876640419948\n",
      "Log Loss:  6.249081312107145\n"
     ]
    }
   ],
   "source": [
    "Motif_present_hist = make_hist(Motif_present_features_array,Motif_present_targetEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "444289a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  93.52014010507881\n",
      "Test score:  3.149606299212598\n",
      "Log Loss:  6.817978785476383\n"
     ]
    }
   ],
   "source": [
    "Motif_number_hist = make_hist(Motif_number_features_array,Motif_number_targetEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0d7de980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  0.7594936708860759\n",
      "Test score:  0.0\n",
      "Log Loss:  22.904112608943016\n"
     ]
    }
   ],
   "source": [
    "Nodes_present_hist = make_hist(Nodes_present_features_array,Nodes_present_targetEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ac38aa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  1.0970464135021099\n",
      "Test score:  0.25252525252525254\n",
      "Log Loss:  22.938631797630553\n"
     ]
    }
   ],
   "source": [
    "Nodes_number_hist = make_hist(Nodes_number_features_array,Nodes_number_targetEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2d4a3fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  67.60070052539405\n",
      "Test score:  6.561679790026247\n",
      "Log Loss:  5.88255239019566\n",
      "Partial accuracy was  34.914463452566096 %\n"
     ]
    }
   ],
   "source": [
    "Motif_present_histmap = make_hist(Motif_present_features_array,Motif_present_targetMap,ncores = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5ddb1683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  69.43957968476357\n",
      "Test score:  3.937007874015748\n",
      "Log Loss:  6.205620534313747\n",
      "Partial accuracy was  33.41384863123994 %\n"
     ]
    }
   ],
   "source": [
    "Motif_number_histmap = make_hist(Motif_number_features_array,Motif_number_targetMap,ncores = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ac6c422f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  14.852320675105485\n",
      "Test score:  5.808080808080808\n",
      "Log Loss:  5.123315969752606\n",
      "Partial accuracy was  36.742147552958365 %\n"
     ]
    }
   ],
   "source": [
    "Nodes_present_histmap = make_hist(Nodes_present_features_array,Nodes_present_targetMap,ncores = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a026e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes_number_histmap = make_hist(Nodes_number_features_array,Nodes_number_targetMap,ncores = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53a68d",
   "metadata": {},
   "source": [
    "Now go ahead and write a function combining all of the above into one that spits out all the information into a table that you can export into R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "184d2a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features, test_features, train_labels, test_labels = train_test_split(features,\n",
    "#                                                                                 labels,\n",
    "#                                                                                 test_size = 0.25, \n",
    "#                                                                                 random_state = 42)\n",
    "def partial_accuracy(test_labels,predictions):\n",
    "    errors = 0\n",
    "    length = 0\n",
    "    for x in range(0,len(predictions)):\n",
    "            map1 = predictions[x]#['path:map00280', 'path:map00630']\n",
    "            map2 = test_labels[x]\n",
    "            map1 = map1.replace('[','').replace(']','').split(',')\n",
    "            map2 = map2.replace('[','').replace(']','').split(',')\n",
    "            total = len(set(map1).intersection(map2))\n",
    "            errors = errors + total\n",
    "            length = length + len(map2)\n",
    "    return(errors/length*100)        \n",
    "    \n",
    "\n",
    "\n",
    "def make_Model(train_features,test_features,train_labels,test_labels,name,predictor,partial = False):\n",
    "    print('Processing ', name, ' Random Forest ',predictor)\n",
    "    start_time = time.time()\n",
    "    ######RandomForest\n",
    "    rf = RandomForestClassifier(random_state = 42,n_estimators = 5000,min_samples_split = 2,min_samples_leaf = 1,max_features = 'sqrt',max_depth = 10,bootstrap = True,oob_score = True)\n",
    "    rf.fit(train_features,train_labels)\n",
    "    acc_rf = rf.score(test_features, test_labels)*100\n",
    "    predictions = rf.predict(test_features)\n",
    "    y_pred = rf.predict_proba(test_features)\n",
    "    lloss_rf = log_loss(test_labels,y_pred,labels=rf.classes_)\n",
    "    conf_mat = confusion_matrix(test_labels, predictions)\n",
    "    FP_rf = conf_mat.sum(axis=0) - np.diag(conf_mat)  \n",
    "    FN_rf = conf_mat.sum(axis=1) - np.diag(conf_mat)\n",
    "    TP_rf = np.diag(conf_mat)\n",
    "    TN_rf = conf_mat.sum() - (FP_rf + FN_rf + TP_rf)\n",
    "    if (partial):\n",
    "        acc_rf = partial_accuracy(test_labels,predictions)\n",
    "    rf_time = time.time() - start_time\n",
    "    print('Took ', rf_time, ' seconds')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print('Processing ', name, ' Ada Boost')\n",
    "    #######Adaboost\n",
    "    ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=9),learning_rate=0.4,n_estimators = 7500)\n",
    "    ada.fit(train_features,train_labels)\n",
    "    acc_ada = ada.score(test_features, test_labels)*100\n",
    "    predictions = ada.predict(test_features)\n",
    "    y_pred = ada.predict_proba(test_features)\n",
    "    lloss_ada = log_loss(test_labels,y_pred,labels=ada.classes_)\n",
    "    conf_mat = confusion_matrix(test_labels, predictions)\n",
    "    FP_ada = conf_mat.sum(axis=0) - np.diag(conf_mat)  \n",
    "    FN_ada = conf_mat.sum(axis=1) - np.diag(conf_mat)\n",
    "    TP_ada = np.diag(conf_mat)\n",
    "    TN_ada = conf_mat.sum() - (FP_ada + FN_ada + TP_ada)\n",
    "    if (partial):\n",
    "        acc_ada = partial_accuracy(test_labels,predictions)\n",
    "    ada_time = time.time() - start_time\n",
    "    print('Took ', ada_time, ' seconds')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print('Processing ', name, ' Gradient Boost')\n",
    "    ############Gradient Boost\n",
    "    hist = HistGradientBoostingClassifier(learning_rate = 0.1, max_depth = 1, max_iter = 100, max_leaf_nodes = 40, min_samples_leaf = 20)\n",
    "    hist.fit(train_features,train_labels)\n",
    "    acc_hist = hist.score(test_features, test_labels)*100\n",
    "    predictions = hist.predict(test_features)\n",
    "    y_pred = hist.predict_proba(test_features)\n",
    "    lloss_hist = log_loss(test_labels,y_pred,labels=hist.classes_)\n",
    "    conf_mat = confusion_matrix(test_labels, predictions)\n",
    "    FP_hist = conf_mat.sum(axis=0) - np.diag(conf_mat)  \n",
    "    FN_hist = conf_mat.sum(axis=1) - np.diag(conf_mat)\n",
    "    TP_hist = np.diag(conf_mat)\n",
    "    TN_hist = conf_mat.sum() - (FP_hist + FN_hist + TP_hist)\n",
    "    if (partial):\n",
    "        acc_rf = partial_accuracy(test_labels,predictions)\n",
    "    \n",
    "    hist_time = time.time() - start_time\n",
    "    print('Took ', rf_time, ' seconds')\n",
    "    \n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame({'Model': ['Random Forest','Ada Boosting','Gradient Boosting'],\n",
    "                       'Result': [acc_rf,acc_ada,acc_hist],\n",
    "                       'Loss': [lloss_rf,lloss_ada,lloss_hist],\n",
    "                       'FP': [FP_rf.mean(),FP_ada.mean(),FP_hist.mean()],\n",
    "                       'FN': [FN_rf.mean(),FN_ada.mean(),FN_hist.mean()],\n",
    "                       'TP': [TP_rf.mean(),TP_ada.mean(),TP_hist.mean()],\n",
    "                       'TN': [TN_rf.mean(),TN_ada.mean(),TN_hist.mean()],\n",
    "                       'Time': [rf_time,ada_time,hist_time],\n",
    "                       'Method': [name,name,name],\n",
    "                       'Predictor': [predictor,predictor,predictor]})\n",
    "    print('Finished ',name)\n",
    "    return(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fd9712a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  Motif_number  Random Forest  ec_id\n",
      "Took  21.47507929801941  seconds\n",
      "Processing  Motif_number  Ada Boost\n",
      "Took  360.5186803340912  seconds\n",
      "Processing  Motif_number  Gradient Boost\n",
      "Took  21.47507929801941  seconds\n",
      "Finished  Motif_number\n",
      "Processing  Motif_number  Random Forest  map_id\n",
      "Took  14.717329740524292  seconds\n",
      "Processing  Motif_number  Ada Boost\n",
      "Took  170.43010902404785  seconds\n",
      "Processing  Motif_number  Gradient Boost\n",
      "Took  14.717329740524292  seconds\n",
      "Finished  Motif_number\n",
      "Processing  Motif_number  Random Forest  partial\n",
      "Took  14.64791488647461  seconds\n",
      "Processing  Motif_number  Ada Boost\n",
      "Took  170.6636209487915  seconds\n",
      "Processing  Motif_number  Gradient Boost\n",
      "Took  14.64791488647461  seconds\n",
      "Finished  Motif_number\n"
     ]
    }
   ],
   "source": [
    "#Do all for Motif Number first\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(Motif_number_features_array,\n",
    "                                                                                Motif_number_targetEC,\n",
    "                                                                                test_size = 0.25, \n",
    "                                                                                random_state = 42)\n",
    "Motif_Number_ecresult = make_Model(train_features,test_features,train_labels,test_labels,'Motif_number','ec_id')\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(Motif_number_features_array,\n",
    "                                                                                Motif_number_targetMap,\n",
    "                                                                                test_size = 0.25, \n",
    "                                                                                random_state = 42)\n",
    "Motif_Number_mapresult = make_Model(train_features,test_features,train_labels,test_labels,'Motif_number','map_id')\n",
    "Motif_Number_partialresult = make_Model(train_features,test_features,train_labels,test_labels,'Motif_number','partial',partial = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e520839",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1208590011.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [30]\u001b[0;36m\u001b[0m\n\u001b[0;31m    Motif_Present_partialresult = make_Model(train_features,test_features,train_labels,test_labels,'Motif_present','partial',,partial = True)\u001b[0m\n\u001b[0m                                                                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Do all for Motif Number first\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(Motif_present_features_array,\n",
    "                                                                                Motif_present_targetEC,\n",
    "                                                                                test_size = 0.25, \n",
    "                                                                                random_state = 42)\n",
    "Motif_Present_ecresult = make_Model(train_features,test_features,train_labels,test_labels,'Motif_present','ec_id')\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(Motif_present_features_array,\n",
    "                                                                                Motif_present_targetMap,\n",
    "                                                                                test_size = 0.25, \n",
    "                                                                                random_state = 42)\n",
    "Motif_Present_mapresult = make_Model(train_features,test_features,train_labels,test_labels,'Motif_present','map_id')\n",
    "Motif_Present_partialresult = make_Model(train_features,test_features,train_labels,test_labels,'Motif_present','partial',,partial = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eb06c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do all for Motif Number first\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(Nodes_present_features_array,\n",
    "                                                                                Nodes_present_targetEC,\n",
    "                                                                                test_size = 0.25, \n",
    "                                                                                random_state = 42)\n",
    "Nodes_Present_ecresult = make_Model(train_features,test_features,train_labels,test_labels,'Nodes_present','ec_id')\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(Nodes_present_features_array,\n",
    "                                                                                Nodes_present_targetMap,\n",
    "                                                                                test_size = 0.25, \n",
    "                                                                                random_state = 42)\n",
    "Nodes_Present_mapresult = make_Model(train_features,test_features,train_labels,test_labels,'Nodes_present','map_id')\n",
    "Nodes_Present_partialresult = make_Model(train_features,test_features,train_labels,test_labels,'Nodes_present','partial',partial = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7bcf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do all for Motif Number first\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(Nodes_number_features_array,\n",
    "                                                                                Nodes_number_targetEC,\n",
    "                                                                                test_size = 0.25, \n",
    "                                                                                random_state = 42)\n",
    "Nodes_Number_ecresult = make_Model(train_features,test_features,train_labels,test_labels,'Nodes_number','ec_id')\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(Nodes_number_features_array,\n",
    "                                                                                Nodes_number_targetMap,\n",
    "                                                                                test_size = 0.25, \n",
    "                                                                                random_state = 42)\n",
    "Nodes_Number_mapresult = make_Model(train_features,test_features,train_labels,test_labels,'Nodes_number','map_id')\n",
    "Nodes_Number_partialresult = make_Model(train_features,test_features,train_labels,test_labels,'Nodes_number','partial',partial = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d607af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_results_df = pd.concat([Motif_Number_ecresult,Motif_Number_mapresult,Motif_Number_partialresult,\n",
    "                 Motif_Present_ecresult,Motif_Present_mapresult,Motif_Present_partialresult,\n",
    "                 Nodes_Number_ecresult,Nodes_Number_mapresult,Nodes_Number_partialresult,\n",
    "                 Nodes_Present_ecresult,Nodes_Present_mapresult,Nodes_Present_partialresult,])\n",
    "print(Train_results_df)\n",
    "Train_results_df.to_csv('/home/robbenm/LuberLab/crispri/EcoBisTest/Training_results_Ensemble.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a15ba2",
   "metadata": {},
   "source": [
    "Now we can move on the the actual test set in which we will try to classify a set of annotated genes from smn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0dd55553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_id</th>\n",
       "      <th>protein_id</th>\n",
       "      <th>ec_id</th>\n",
       "      <th>species</th>\n",
       "      <th>map_id</th>\n",
       "      <th>MEME-1</th>\n",
       "      <th>MEME-2</th>\n",
       "      <th>MEME-3</th>\n",
       "      <th>MEME-4</th>\n",
       "      <th>MEME-5</th>\n",
       "      <th>...</th>\n",
       "      <th>MEME-88</th>\n",
       "      <th>MEME-89</th>\n",
       "      <th>MEME-90</th>\n",
       "      <th>MEME-91</th>\n",
       "      <th>MEME-92</th>\n",
       "      <th>MEME-93</th>\n",
       "      <th>MEME-94</th>\n",
       "      <th>MEME-95</th>\n",
       "      <th>MEME-96</th>\n",
       "      <th>MEME-97</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smn:SMA_0012</td>\n",
       "      <td>ncbi-proteinid:CCF01303</td>\n",
       "      <td>ec:3.5.2.6</td>\n",
       "      <td>smn</td>\n",
       "      <td>['path:map00311', 'path:map01110']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>smn:SMA_0014</td>\n",
       "      <td>ncbi-proteinid:CCF01305</td>\n",
       "      <td>ec:2.4.2.8</td>\n",
       "      <td>smn</td>\n",
       "      <td>['path:map00230', 'path:map00983', 'path:map01...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>smn:SMA_0023</td>\n",
       "      <td>ncbi-proteinid:CCF01314</td>\n",
       "      <td>ec:2.7.6.1</td>\n",
       "      <td>smn</td>\n",
       "      <td>['path:map00030', 'path:map00230', 'path:map01...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>smn:SMA_0026</td>\n",
       "      <td>ncbi-proteinid:CCF01317</td>\n",
       "      <td>ec:2.3.1.274</td>\n",
       "      <td>smn</td>\n",
       "      <td>['path:map00561', 'path:map01100']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>smn:SMA_0028</td>\n",
       "      <td>ncbi-proteinid:CCF01319</td>\n",
       "      <td>ec:6.3.2.6</td>\n",
       "      <td>smn</td>\n",
       "      <td>['path:map00230', 'path:map01100', 'path:map01...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        gene_id               protein_id         ec_id species  \\\n",
       "0  smn:SMA_0012  ncbi-proteinid:CCF01303    ec:3.5.2.6     smn   \n",
       "1  smn:SMA_0014  ncbi-proteinid:CCF01305    ec:2.4.2.8     smn   \n",
       "2  smn:SMA_0023  ncbi-proteinid:CCF01314    ec:2.7.6.1     smn   \n",
       "3  smn:SMA_0026  ncbi-proteinid:CCF01317  ec:2.3.1.274     smn   \n",
       "4  smn:SMA_0028  ncbi-proteinid:CCF01319    ec:6.3.2.6     smn   \n",
       "\n",
       "                                              map_id  MEME-1  MEME-2  MEME-3  \\\n",
       "0                 ['path:map00311', 'path:map01110']       0       0       0   \n",
       "1  ['path:map00230', 'path:map00983', 'path:map01...       0       0       0   \n",
       "2  ['path:map00030', 'path:map00230', 'path:map01...       1       1       0   \n",
       "3                 ['path:map00561', 'path:map01100']       0       0       0   \n",
       "4  ['path:map00230', 'path:map01100', 'path:map01...       0       0       0   \n",
       "\n",
       "   MEME-4  MEME-5  ...  MEME-88  MEME-89  MEME-90  MEME-91  MEME-92  MEME-93  \\\n",
       "0       0       0  ...        0        0        0        0        1        0   \n",
       "1       0       0  ...        0        0        0        0        1        0   \n",
       "2       0       0  ...        0        0        0        0        0        0   \n",
       "3       0       0  ...        0        0        0        0        0        0   \n",
       "4       0       0  ...        0        0        0        0        0        0   \n",
       "\n",
       "   MEME-94  MEME-95  MEME-96  MEME-97  \n",
       "0        0        0        0        0  \n",
       "1        0        0        0        0  \n",
       "2        0        0        0        0  \n",
       "3        0        0        0        0  \n",
       "4        0        0        0        0  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in the new datasets\n",
    "Motif_present_smn = pd.read_csv('/home/robbenm/LuberLab/crispri/EcoBisTest/SMN_Test/Motif_Present_smn.tsv',sep=\"\\t\")\n",
    "Motif_number_smn = pd.read_csv('/home/robbenm/LuberLab/crispri/EcoBisTest/SMN_Test/Motif_Number_smn.tsv',sep=\"\\t\")\n",
    "Nodes_present_smn = pd.read_csv('/home/robbenm/LuberLab/crispri/EcoBisTest/SMN_Test/Nodes_Present_smn.tsv',sep=\"\\t\")\n",
    "Nodes_number_smn = pd.read_csv('/home/robbenm/LuberLab/crispri/EcoBisTest/SMN_Test/Nodes_Number_smn.tsv',sep=\"\\t\")\n",
    "Motif_number_smn.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f34ab3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Motif_present_targetEC_smn = np.array(Motif_present_smn['ec_id']) #Lets start with predicting the EC and then we can try mapID later\n",
    "Motif_number_targetEC_smn = np.array(Motif_number_smn['ec_id'])\n",
    "Nodes_present_targetEC_smn = np.array(Nodes_present_smn['ec_id'])\n",
    "Nodes_number_targetEC_smn = np.array(Nodes_number_smn['ec_id'])\n",
    "Motif_number_targetEC_smn.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "505efc1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Motif_present_targetMap_smn = np.array(Motif_present_smn['map_id'])\n",
    "Motif_number_targetMap_smn = np.array(Motif_number_smn['map_id'])\n",
    "Nodes_present_targetMap_smn = np.array(Nodes_present_smn['map_id'])\n",
    "Nodes_number_targetMap_smn = np.array(Nodes_number_smn['map_id'])\n",
    "Motif_present_targetMap_smn.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3467c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns from feature array\n",
    "colDrop = ['gene_id','protein_id','ec_id','species','map_id']\n",
    "Motif_present_features_smn = Motif_present_smn.drop(colDrop,axis = 1)\n",
    "Motif_number_features_smn = Motif_number_smn.drop(colDrop,axis = 1)\n",
    "Nodes_present_features_smn = Nodes_present_smn.drop(colDrop,axis = 1)\n",
    "Nodes_number_features_smn = Nodes_number_smn.drop(colDrop,axis = 1)\n",
    "#convert to array\n",
    "Motif_present_features_array_smn = np.array(Motif_present_features_smn)\n",
    "Motif_number_features_array_smn = np.array(Motif_number_features_smn)\n",
    "Nodes_present_features_array_smn = np.array(Nodes_present_features_smn)\n",
    "Nodes_number_features_array_smn = np.array(Nodes_number_features_smn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cbddef",
   "metadata": {},
   "source": [
    "Now that we have these in we can run the models using all the ecobis set as training data and all the smn set as a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93bb8653",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  Motif_number  Random Forest  ec_id\n",
      "Took  27.505934476852417  seconds\n",
      "Processing  Motif_number  Ada Boost\n",
      "Took  522.4465916156769  seconds\n",
      "Processing  Motif_number  Gradient Boost\n",
      "Took  27.505934476852417  seconds\n",
      "Finished  Motif_number\n",
      "Processing  Motif_number  Random Forest  map_id\n",
      "Took  16.041162729263306  seconds\n",
      "Processing  Motif_number  Ada Boost\n",
      "Took  221.15613627433777  seconds\n",
      "Processing  Motif_number  Gradient Boost\n",
      "Took  16.041162729263306  seconds\n",
      "Finished  Motif_number\n",
      "Processing  Motif_number  Random Forest  partial\n",
      "Took  16.250980615615845  seconds\n",
      "Processing  Motif_number  Ada Boost\n",
      "Took  222.3646776676178  seconds\n",
      "Processing  Motif_number  Gradient Boost\n",
      "Took  16.250980615615845  seconds\n",
      "Finished  Motif_number\n",
      "Processing  Motif_present  Random Forest  ec_id\n",
      "Took  27.224644660949707  seconds\n",
      "Processing  Motif_present  Ada Boost\n",
      "Took  495.9595637321472  seconds\n",
      "Processing  Motif_present  Gradient Boost\n",
      "Took  27.224644660949707  seconds\n",
      "Finished  Motif_present\n",
      "Processing  Motif_present  Random Forest  map_id\n",
      "Took  16.070234298706055  seconds\n",
      "Processing  Motif_present  Ada Boost\n",
      "Took  220.879656791687  seconds\n",
      "Processing  Motif_present  Gradient Boost\n",
      "Took  16.070234298706055  seconds\n",
      "Finished  Motif_present\n",
      "Processing  Motif_present  Random Forest  partial\n",
      "Took  16.009586811065674  seconds\n",
      "Processing  Motif_present  Ada Boost\n",
      "Took  221.2297704219818  seconds\n",
      "Processing  Motif_present  Gradient Boost\n",
      "Took  16.009586811065674  seconds\n",
      "Finished  Motif_present\n",
      "Processing  Nodes_present  Random Forest  ec_id\n",
      "Took  65.15309453010559  seconds\n",
      "Processing  Nodes_present  Ada Boost\n",
      "Took  1682.340732574463  seconds\n",
      "Processing  Nodes_present  Gradient Boost\n",
      "Took  65.15309453010559  seconds\n",
      "Finished  Nodes_present\n",
      "Processing  Nodes_present  Random Forest  map_id\n",
      "Took  56.27206635475159  seconds\n",
      "Processing  Nodes_present  Ada Boost\n",
      "Took  1262.5812292099  seconds\n",
      "Processing  Nodes_present  Gradient Boost\n",
      "Took  56.27206635475159  seconds\n",
      "Finished  Nodes_present\n",
      "Processing  Nodes_present  Random Forest  partial\n",
      "Took  54.05510640144348  seconds\n",
      "Processing  Nodes_present  Ada Boost\n",
      "Took  1271.111927986145  seconds\n",
      "Processing  Nodes_present  Gradient Boost\n",
      "Took  54.05510640144348  seconds\n",
      "Finished  Nodes_present\n",
      "Processing  Nodes_number  Random Forest  ec_id\n",
      "Took  66.10817313194275  seconds\n",
      "Processing  Nodes_number  Ada Boost\n",
      "Took  2009.0545177459717  seconds\n",
      "Processing  Nodes_number  Gradient Boost\n",
      "Took  66.10817313194275  seconds\n",
      "Finished  Nodes_number\n",
      "Processing  Nodes_number  Random Forest  map_id\n",
      "Took  58.48592400550842  seconds\n",
      "Processing  Nodes_number  Ada Boost\n",
      "Took  1444.4919238090515  seconds\n",
      "Processing  Nodes_number  Gradient Boost\n",
      "Took  58.48592400550842  seconds\n",
      "Finished  Nodes_number\n",
      "Processing  Nodes_number  Random Forest  partial\n",
      "Took  55.49574565887451  seconds\n",
      "Processing  Nodes_number  Ada Boost\n",
      "Took  1430.1485052108765  seconds\n",
      "Processing  Nodes_number  Gradient Boost\n",
      "Took  55.49574565887451  seconds\n",
      "Finished  Nodes_number\n"
     ]
    }
   ],
   "source": [
    "#Do all for Motif Number first\n",
    "Motif_Number_ecresult = make_Model(Motif_number_features_array,Motif_number_features_array_smn,Motif_number_targetEC,Motif_number_targetEC_smn,'Motif_number','ec_id')\n",
    "Motif_Number_mapresult = make_Model(Motif_number_features_array,Motif_number_features_array_smn,Motif_number_targetMap,Motif_number_targetMap_smn,'Motif_number','map_id')\n",
    "Motif_Number_partialresult = make_Model(Motif_number_features_array,Motif_number_features_array_smn,Motif_number_targetMap,Motif_number_targetMap_smn,'Motif_number','partial',partial = True)\n",
    "#Do all for Motif Present\n",
    "Motif_Present_ecresult = make_Model(Motif_present_features_array,Motif_present_features_array_smn,Motif_present_targetEC,Motif_present_targetEC_smn,'Motif_present','ec_id')\n",
    "Motif_Present_mapresult = make_Model(Motif_present_features_array,Motif_present_features_array_smn,Motif_present_targetMap,Motif_present_targetMap_smn,'Motif_present','map_id')\n",
    "Motif_Present_partialresult = make_Model(Motif_present_features_array,Motif_present_features_array_smn,Motif_present_targetMap,Motif_present_targetMap_smn,'Motif_present','partial',partial = True)\n",
    "#Do all for Nodes Present\n",
    "Nodes_Present_ecresult = make_Model(Nodes_present_features_array,Nodes_present_features_array_smn,Nodes_present_targetEC,Nodes_present_targetEC_smn,'Nodes_present','ec_id')\n",
    "Nodes_Present_mapresult = make_Model(Nodes_present_features_array,Nodes_present_features_array_smn,Nodes_present_targetMap,Nodes_present_targetMap_smn,'Nodes_present','map_id')\n",
    "Nodes_Present_partialresult = make_Model(Nodes_present_features_array,Nodes_present_features_array_smn,Nodes_present_targetMap,Nodes_present_targetMap_smn,'Nodes_present','partial',partial = True)\n",
    "#Do all for Nodes Number\n",
    "Nodes_Number_ecresult = make_Model(Nodes_number_features_array,Nodes_number_features_array_smn,Nodes_number_targetEC,Nodes_number_targetEC_smn,'Nodes_number','ec_id')\n",
    "Nodes_Number_mapresult = make_Model(Nodes_number_features_array,Nodes_number_features_array_smn,Nodes_number_targetMap,Nodes_number_targetMap_smn,'Nodes_number','map_id')\n",
    "Nodes_Number_partialresult = make_Model(Nodes_number_features_array,Nodes_number_features_array_smn,Nodes_number_targetMap,Nodes_number_targetMap_smn,'Nodes_number','partial',partial = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecb19740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model     Result       Loss        FP        FN        TP  \\\n",
      "0      Random Forest   0.000000   5.539220  1.005650  1.005650  0.000000   \n",
      "1       Ada Boosting   0.000000   8.393204  0.874693  0.874693  0.000000   \n",
      "2  Gradient Boosting   0.561798   8.126514  0.793722  0.793722  0.004484   \n",
      "0      Random Forest   0.561798   4.785904  1.945055  1.945055  0.010989   \n",
      "1       Ada Boosting   0.842697   6.756687  1.773869  1.773869  0.015075   \n",
      "2  Gradient Boosting   0.561798   5.584271  1.539130  1.539130  0.008696   \n",
      "0      Random Forest  28.778822   4.785904  1.945055  1.945055  0.010989   \n",
      "1       Ada Boosting  28.522630   6.754840  1.786802  1.786802  0.020305   \n",
      "2  Gradient Boosting   0.561798   5.584271  1.539130  1.539130  0.008696   \n",
      "0      Random Forest   0.000000   5.542554  1.017143  1.017143  0.000000   \n",
      "1       Ada Boosting   0.000000   8.396381  0.874693  0.874693  0.000000   \n",
      "2  Gradient Boosting   0.561798   8.126514  0.791946  0.791946  0.004474   \n",
      "0      Random Forest   0.561798   4.786234  1.945055  1.945055  0.010989   \n",
      "1       Ada Boosting   0.842697   6.757194  1.782828  1.782828  0.015152   \n",
      "2  Gradient Boosting   0.561798   5.584271  1.539130  1.539130  0.008696   \n",
      "0      Random Forest  28.778822   4.786234  1.945055  1.945055  0.010989   \n",
      "1       Ada Boosting  28.181042   6.761963  1.791878  1.791878  0.015228   \n",
      "2  Gradient Boosting   0.561798   5.584271  1.539130  1.539130  0.008696   \n",
      "0      Random Forest   0.280899   5.482895  1.102484  1.102484  0.003106   \n",
      "1       Ada Boosting   0.561798   5.539403  0.967213  0.967213  0.005464   \n",
      "2  Gradient Boosting   0.561798  28.620615  1.099379  1.099379  0.006211   \n",
      "0      Random Forest   0.280899   4.741489  1.961326  1.961326  0.005525   \n",
      "1       Ada Boosting   0.842697   5.006244  1.867725  1.867725  0.015873   \n",
      "2  Gradient Boosting   0.000000  29.978882  2.069767  2.069767  0.000000   \n",
      "0      Random Forest   0.085397   4.741489  1.961326  1.961326  0.005525   \n",
      "1       Ada Boosting  23.313407   5.004864  1.867725  1.867725  0.015873   \n",
      "2  Gradient Boosting   0.000000  29.978882  2.069767  2.069767  0.000000   \n",
      "0      Random Forest   0.842697   5.576234  1.020231  1.020231  0.008671   \n",
      "1       Ada Boosting   0.280899   5.509659  0.806818  0.806818  0.002273   \n",
      "2  Gradient Boosting   0.280899  28.629611  1.123418  1.123418  0.003165   \n",
      "0      Random Forest   0.842697   4.857894  1.848168  1.848168  0.015707   \n",
      "1       Ada Boosting   0.000000   4.918404  1.508475  1.508475  0.000000   \n",
      "2  Gradient Boosting   3.089888  28.824431  2.029412  2.029412  0.064706   \n",
      "0      Random Forest  40.478224   4.857894  1.848168  1.848168  0.015707   \n",
      "1       Ada Boosting  32.023911   4.931417  1.477178  1.477178  0.000000   \n",
      "2  Gradient Boosting   3.089888  28.824431  2.017544  2.017544  0.064327   \n",
      "\n",
      "           TN         Time         Method Predictor  \n",
      "0  353.988701    27.505934   Motif_number     ec_id  \n",
      "1  354.250614   522.446592   Motif_number     ec_id  \n",
      "2  354.408072   338.237228   Motif_number     ec_id  \n",
      "0  352.098901    16.041163   Motif_number    map_id  \n",
      "1  352.437186   221.156136   Motif_number    map_id  \n",
      "2  352.913043  1149.777246   Motif_number    map_id  \n",
      "0  352.098901    16.250981   Motif_number   partial  \n",
      "1  352.406091   222.364678   Motif_number   partial  \n",
      "2  352.913043   807.459713   Motif_number   partial  \n",
      "0  353.965714    27.224645  Motif_present     ec_id  \n",
      "1  354.250614   495.959564  Motif_present     ec_id  \n",
      "2  354.411633  3549.891867  Motif_present     ec_id  \n",
      "0  352.098901    16.070234  Motif_present    map_id  \n",
      "1  352.419192   220.879657  Motif_present    map_id  \n",
      "2  352.913043   130.995081  Motif_present    map_id  \n",
      "0  352.098901    16.009587  Motif_present   partial  \n",
      "1  352.401015   221.229770  Motif_present   partial  \n",
      "2  352.913043   100.237617  Motif_present   partial  \n",
      "0  353.791925    66.108173   Nodes_number     ec_id  \n",
      "1  354.060109  2009.054518   Nodes_number     ec_id  \n",
      "2  353.795031   739.692007   Nodes_number     ec_id  \n",
      "0  352.071823    58.485924   Nodes_number    map_id  \n",
      "1  352.248677  1444.491924   Nodes_number    map_id  \n",
      "2  351.860465   311.973616   Nodes_number    map_id  \n",
      "0  352.071823    55.495746   Nodes_number   partial  \n",
      "1  352.248677  1430.148505   Nodes_number   partial  \n",
      "2  351.860465   475.828555   Nodes_number   partial  \n",
      "0  353.950867    65.153095  Nodes_present     ec_id  \n",
      "1  354.384091  1682.340733  Nodes_present     ec_id  \n",
      "2  353.750000   187.447236  Nodes_present     ec_id  \n",
      "0  352.287958    56.272066  Nodes_present    map_id  \n",
      "1  352.983051  1262.581229  Nodes_present    map_id  \n",
      "2  351.876471   169.702275  Nodes_present    map_id  \n",
      "0  352.287958    54.055106  Nodes_present   partial  \n",
      "1  353.045643  1271.111928  Nodes_present   partial  \n",
      "2  351.900585   339.655218  Nodes_present   partial  \n"
     ]
    }
   ],
   "source": [
    "Test_results_df = pd.concat([Motif_Number_ecresult,Motif_Number_mapresult,Motif_Number_partialresult,\n",
    "                 Motif_Present_ecresult,Motif_Present_mapresult,Motif_Present_partialresult,\n",
    "                 Nodes_Number_ecresult,Nodes_Number_mapresult,Nodes_Number_partialresult,\n",
    "                 Nodes_Present_ecresult,Nodes_Present_mapresult,Nodes_Present_partialresult,])\n",
    "print(Test_results_df)\n",
    "Test_results_df.to_csv('/home/robbenm/LuberLab/crispri/EcoBisTest/Testing_results_Ensemble.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d583c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestClassifier(random_state = 42,\n",
    "#                                 n_estimators = 5000,\n",
    "#                                 min_samples_split = 2,\n",
    "#                                 min_samples_leaf = 1,\n",
    "#                                 max_features = 'sqrt',\n",
    "#                                 max_depth = 10,\n",
    "#                                 bootstrap = True)\n",
    "# rf.fit(Motif_number_features_array,Motif_number_targetEC)\n",
    "# predictions = rf.predict(Motif_number_features_array_smn)\n",
    "#Motif_number_features_array,Motif_number_features_array_smn,Motif_number_targetEC,Motif_number_targetEC_smn,'Motif_number','ec_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2365d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features, test_features, train_labels, test_labels = train_test_split(features,\n",
    "#                                                                                 labels,\n",
    "#                                                                                 test_size = 0.25, \n",
    "#                                                                                 random_state = 42)\n",
    "def partial_accuracy(test_labels,predictions):\n",
    "    errors = 0\n",
    "    length = 0\n",
    "    for x in range(0,len(predictions)):\n",
    "            map1 = predictions[x]#['path:map00280', 'path:map00630']\n",
    "            map2 = test_labels[x]\n",
    "            map1 = map1.replace('[','').replace(']','').split(',')\n",
    "            map2 = map2.replace('[','').replace(']','').split(',')\n",
    "            total = len(set(map1).intersection(map2))\n",
    "            errors = errors + total\n",
    "            length = length + len(map2)\n",
    "    return(errors/length*100)        \n",
    "    \n",
    "\n",
    "\n",
    "def make_Model(train_features,test_features,train_labels,test_labels,name,predictor,partial = False):\n",
    "    print('Processing ', name, ' Random Forest ',predictor)\n",
    "    start_time = time.time()\n",
    "    ######RandomForest\n",
    "    rf = RandomForestClassifier(random_state = 42,n_estimators = 5000,min_samples_split = 2,min_samples_leaf = 1,max_features = 'sqrt',max_depth = 10,bootstrap = True,oob_score = True)\n",
    "    rf.fit(train_features,train_labels)\n",
    "    acc_rf = rf.score(test_features, test_labels)*100\n",
    "    predictions_rf = rf.predict(test_features)\n",
    "    y_pred = rf.predict_proba(test_features)\n",
    "    lloss_rf = log_loss(test_labels,y_pred,labels=rf.classes_)\n",
    "    conf_mat = confusion_matrix(test_labels, predictions_rf)\n",
    "    FP_rf = conf_mat.sum(axis=0) - np.diag(conf_mat)  \n",
    "    FN_rf = conf_mat.sum(axis=1) - np.diag(conf_mat)\n",
    "    TP_rf = np.diag(conf_mat)\n",
    "    TN_rf = conf_mat.sum() - (FP_rf + FN_rf + TP_rf)\n",
    "    if (partial):\n",
    "        acc_rf = partial_accuracy(test_labels,predictions_rf)\n",
    "    rf_time = time.time() - start_time\n",
    "    print('Took ', rf_time, ' seconds')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print('Processing ', name, ' Ada Boost')\n",
    "    #######Adaboost\n",
    "    ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=9),learning_rate=0.4,n_estimators = 7500)\n",
    "    ada.fit(train_features,train_labels)\n",
    "    acc_ada = ada.score(test_features, test_labels)*100\n",
    "    predictions_ada = ada.predict(test_features)\n",
    "    y_pred = ada.predict_proba(test_features)\n",
    "    lloss_ada = log_loss(test_labels,y_pred,labels=ada.classes_)\n",
    "    conf_mat = confusion_matrix(test_labels, predictions_ada)\n",
    "    FP_ada = conf_mat.sum(axis=0) - np.diag(conf_mat)  \n",
    "    FN_ada = conf_mat.sum(axis=1) - np.diag(conf_mat)\n",
    "    TP_ada = np.diag(conf_mat)\n",
    "    TN_ada = conf_mat.sum() - (FP_ada + FN_ada + TP_ada)\n",
    "    if (partial):\n",
    "        acc_ada = partial_accuracy(test_labels,predictions_ada)\n",
    "    ada_time = time.time() - start_time\n",
    "    print('Took ', ada_time, ' seconds')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print('Processing ', name, ' Gradient Boost')\n",
    "    ############Gradient Boost\n",
    "    hist = HistGradientBoostingClassifier(learning_rate = 0.1, max_depth = 1, max_iter = 100, max_leaf_nodes = 40, min_samples_leaf = 20)\n",
    "    hist.fit(train_features,train_labels)\n",
    "    acc_hist = hist.score(test_features, test_labels)*100\n",
    "    predictions_hist = hist.predict(test_features)\n",
    "    y_pred = hist.predict_proba(test_features)\n",
    "    lloss_hist = log_loss(test_labels,y_pred,labels=hist.classes_)\n",
    "    conf_mat = confusion_matrix(test_labels, predictions_hist)\n",
    "    FP_hist = conf_mat.sum(axis=0) - np.diag(conf_mat)  \n",
    "    FN_hist = conf_mat.sum(axis=1) - np.diag(conf_mat)\n",
    "    TP_hist = np.diag(conf_mat)\n",
    "    TN_hist = conf_mat.sum() - (FP_hist + FN_hist + TP_hist)\n",
    "    if (partial):\n",
    "        acc_rf = partial_accuracy(test_labels,predictions_hist)\n",
    "    \n",
    "    hist_time = time.time() - start_time\n",
    "    print('Took ', rf_time, ' seconds')\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame({'RFpred': [predictions_rf],\n",
    "                       'Adapred': [predictions_ada],\n",
    "                       'Histpred': [predictions_hist]})\n",
    "    print('Finished ',name)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "94c54dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  Motif_number  Random Forest  ec_id\n",
      "Took  28.683926582336426  seconds\n",
      "Processing  Motif_number  Ada Boost\n",
      "Took  515.5483901500702  seconds\n",
      "Processing  Motif_number  Gradient Boost\n",
      "Took  28.683926582336426  seconds\n",
      "Finished  Motif_number\n",
      "Processing  Motif_present  Random Forest  ec_id\n",
      "Took  27.68413996696472  seconds\n",
      "Processing  Motif_present  Ada Boost\n",
      "Took  503.62461376190186  seconds\n",
      "Processing  Motif_present  Gradient Boost\n",
      "Took  27.68413996696472  seconds\n",
      "Finished  Motif_present\n",
      "Processing  Nodes_present  Random Forest  ec_id\n",
      "Took  68.85148596763611  seconds\n",
      "Processing  Nodes_present  Ada Boost\n",
      "Took  1682.8307247161865  seconds\n",
      "Processing  Nodes_present  Gradient Boost\n",
      "Took  68.85148596763611  seconds\n",
      "Finished  Nodes_present\n",
      "Processing  Nodes_number  Random Forest  ec_id\n",
      "Took  71.86184525489807  seconds\n",
      "Processing  Nodes_number  Ada Boost\n",
      "Took  2008.9879524707794  seconds\n",
      "Processing  Nodes_number  Gradient Boost\n",
      "Took  71.86184525489807  seconds\n",
      "Finished  Nodes_number\n"
     ]
    }
   ],
   "source": [
    "#Do all for Motif Number first\n",
    "Motif_Number_ecresult = make_Model(Motif_number_features_array,Motif_number_features_array_smn,Motif_number_targetEC,Motif_number_targetEC_smn,'Motif_number','ec_id')\n",
    "#Motif_Number_mapresult = make_Model(Motif_number_features_array,Motif_number_features_array_smn,Motif_number_targetMap,Motif_number_targetMap_smn,'Motif_number','map_id')\n",
    "#Motif_Number_partialresult = make_Model(Motif_number_features_array,Motif_number_features_array_smn,Motif_number_targetMap,Motif_number_targetMap_smn,'Motif_number','partial',partial = True)\n",
    "#Do all for Motif Present\n",
    "Motif_Present_ecresult = make_Model(Motif_present_features_array,Motif_present_features_array_smn,Motif_present_targetEC,Motif_present_targetEC_smn,'Motif_present','ec_id')\n",
    "#Motif_Present_mapresult = make_Model(Motif_present_features_array,Motif_present_features_array_smn,Motif_present_targetMap,Motif_present_targetMap_smn,'Motif_present','map_id')\n",
    "#Motif_Present_partialresult = make_Model(Motif_present_features_array,Motif_present_features_array_smn,Motif_present_targetMap,Motif_present_targetMap_smn,'Motif_present','partial',partial = True)\n",
    "#Do all for Nodes Present\n",
    "Nodes_Present_ecresult = make_Model(Nodes_present_features_array,Nodes_present_features_array_smn,Nodes_present_targetEC,Nodes_present_targetEC_smn,'Nodes_present','ec_id')\n",
    "#Nodes_Present_mapresult = make_Model(Nodes_present_features_array,Nodes_present_features_array_smn,Nodes_present_targetMap,Nodes_present_targetMap_smn,'Nodes_present','map_id')\n",
    "#Nodes_Present_partialresult = make_Model(Nodes_present_features_array,Nodes_present_features_array_smn,Nodes_present_targetMap,Nodes_present_targetMap_smn,'Nodes_present','partial',partial = True)\n",
    "#Do all for Nodes Number\n",
    "Nodes_Number_ecresult = make_Model(Nodes_number_features_array,Nodes_number_features_array_smn,Nodes_number_targetEC,Nodes_number_targetEC_smn,'Nodes_number','ec_id')\n",
    "#Nodes_Number_mapresult = make_Model(Nodes_number_features_array,Nodes_number_features_array_smn,Nodes_number_targetMap,Nodes_number_targetMap_smn,'Nodes_number','map_id')\n",
    "#Nodes_Number_partialresult = make_Model(Nodes_number_features_array,Nodes_number_features_array_smn,Nodes_number_targetMap,Nodes_number_targetMap_smn,'Nodes_number','partial',partial = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83cd0abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              RFpred  \\\n",
      "0  [ec:7.1.1.2, ec:7.1.1.2, ec:7.1.1.2, ec:7.1.1....   \n",
      "0  [ec:7.1.1.2, ec:7.1.1.2, ec:7.1.1.2, ec:7.1.1....   \n",
      "0  [ec:7.1.1.2, ec:7.1.1.2, ec:5.1.3.3, ec:7.1.1....   \n",
      "0  [ec:6.2.1.3, ec:6.2.1.3, ec:3.2.1.23, ec:2.8.1...   \n",
      "\n",
      "                                             Adapred  \\\n",
      "0  [ec:3.5.1.1, ec:3.5.1.1, ec:7.1.1.2, ec:2.5.1....   \n",
      "0  [ec:3.5.1.1, ec:3.5.1.1, ec:7.1.1.2, ec:2.5.1....   \n",
      "0  [ec:7.1.1.2, ec:7.1.1.2, ec:3.6.1.13, ec:4.6.1...   \n",
      "0  [ec:2.8.1.4, ec:2.8.1.4, ec:2.5.1.72, ec:3.2.1...   \n",
      "\n",
      "                                            Histpred  \n",
      "0  [ec:1.4.3.5, ec:1.4.3.5, ec:1.17.1.8, ec:2.5.1...  \n",
      "0  [ec:1.4.3.5, ec:1.4.3.5, ec:1.17.1.8, ec:2.5.1...  \n",
      "0  [ec:2.7.1.200, ec:2.7.1.200, ec:4.1.3.27, ec:3...  \n",
      "0  [ec:2.7.7.24, ec:2.7.7.24, ec:4.2.1.46, ec:2.7...  \n"
     ]
    }
   ],
   "source": [
    "Test_results_df = pd.concat([Motif_Number_ecresult,#Motif_Number_mapresult,Motif_Number_partialresult,\n",
    "                 Motif_Present_ecresult,#Motif_Present_mapresult,Motif_Present_partialresult,\n",
    "                 Nodes_Number_ecresult,#Nodes_Number_mapresult,Nodes_Number_partialresult,\n",
    "                 Nodes_Present_ecresult])#,Nodes_Present_mapresult,Nodes_Present_partialresult,])\n",
    "print(Test_results_df)\n",
    "Test_results_df.to_csv('/home/robbenm/LuberLab/crispri/EcoBisTest/Testing_pred_Ensemble.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fe92f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
